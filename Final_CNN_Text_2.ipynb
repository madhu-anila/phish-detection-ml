{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a7ab48e-5b1a-4593-87d4-ef4bc3251d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.optim import Adam\n",
    "#from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94856d52-c955-485d-b915-2665887175bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw columns: ['email_id', 'sender', 'receiver', 'date', 'subject', 'body', 'urls', 'label', 'source_dataset']\n",
      "    email_id                                             sender  \\\n",
      "0  CEAS_08_0                   young esposito <young@iworld.de>   \n",
      "1  CEAS_08_1                       mok <ipline's1983@icable.ph>   \n",
      "2  CEAS_08_2  daily top 10 <karmandeep-opengevl@universalnet...   \n",
      "3  CEAS_08_3                 michael parker <ivqrnai@pobox.com>   \n",
      "4  CEAS_08_4  gretchen suggs <externalsep1@loanofficertool.com>   \n",
      "\n",
      "                                         receiver  \\\n",
      "0                     user4@gvc.ceas-challenge.cc   \n",
      "1                   user2.2@gvc.ceas-challenge.cc   \n",
      "2                   user2.9@gvc.ceas-challenge.cc   \n",
      "3  spamassassin dev <xrh@spamassassin.apache.org>   \n",
      "4                   user2.2@gvc.ceas-challenge.cc   \n",
      "\n",
      "                              date  \\\n",
      "0  Tue, 05 Aug 2008 16:31:02 -0700   \n",
      "1  Tue, 05 Aug 2008 18:31:03 -0500   \n",
      "2  Tue, 05 Aug 2008 20:28:00 -1200   \n",
      "3  Tue, 05 Aug 2008 17:31:20 -0600   \n",
      "4  Tue, 05 Aug 2008 19:31:21 -0400   \n",
      "\n",
      "                                             subject  \\\n",
      "0                          Never agree to be a loser   \n",
      "1                             Befriend Jenna Jameson   \n",
      "2                               CNN.com Daily Top 10   \n",
      "3  Re: svn commit: r619753 - in /spamassassin/tru...   \n",
      "4                         SpecialPricesPharmMoreinfo   \n",
      "\n",
      "                                                body  urls  label  \\\n",
      "0  Buck up, your troubles caused by small dimensi...   1.0      1   \n",
      "1  Upgrade your sex and pleasures with these tech...   1.0      1   \n",
      "2  >+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+...   1.0      1   \n",
      "3  Would anyone object to removing .so from this ...   1.0      0   \n",
      "4  WelcomeFastShippingCustomerSupport\\nhttp://7iw...   1.0      1   \n",
      "\n",
      "  source_dataset  \n",
      "0        CEAS_08  \n",
      "1        CEAS_08  \n",
      "2        CEAS_08  \n",
      "3        CEAS_08  \n",
      "4        CEAS_08  \n",
      "Dataset size: 76341\n",
      "                                                text  label\n",
      "0  Never agree to be a loser Buck up, your troubl...      1\n",
      "1  Befriend Jenna Jameson Upgrade your sex and pl...      1\n",
      "2  CNN.com Daily Top 10 >+=+=+=+=+=+=+=+=+=+=+=+=...      1\n",
      "3  Re: svn commit: r619753 - in /spamassassin/tru...      0\n",
      "4  SpecialPricesPharmMoreinfo WelcomeFastShipping...      1\n"
     ]
    }
   ],
   "source": [
    "CSV_PATH = \"unified_phishing_dataset.csv\"\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "print(\"Raw columns:\", df.columns.tolist())\n",
    "print(df.head())\n",
    "\n",
    "# Keep only what we need\n",
    "df = df[[\"subject\", \"body\", \"label\"]]\n",
    "\n",
    "# Drop rows with missing text or label\n",
    "df = df.dropna(subset=[\"subject\", \"body\", \"label\"])\n",
    "\n",
    "# Combine subject + body into one text field\n",
    "df[\"text\"] = df[\"subject\"].astype(str) + \" \" + df[\"body\"].astype(str)\n",
    "\n",
    "# Make sure labels are ints (0 or 1)\n",
    "df[\"label\"] = df[\"label\"].astype(int)\n",
    "\n",
    "print(\"Dataset size:\", len(df))\n",
    "print(df[[\"text\", \"label\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c5dd7a8-16da-40e6-9a36-d8d01692328b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 61072\n",
      "Val size: 15269\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>body</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35921</th>\n",
       "      <td>CNN Alerts: My Custom Alert</td>\n",
       "      <td>CNN Alerts: My Custom Alert\\n\\n\\n\\n\\n\\n\\n \\n\\n...</td>\n",
       "      <td>1</td>\n",
       "      <td>CNN Alerts: My Custom Alert CNN Alerts: My Cus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5507</th>\n",
       "      <td>You do not want to buy their shops unknown in ...</td>\n",
       "      <td>Man's improving formula effective for for 90% ...</td>\n",
       "      <td>1</td>\n",
       "      <td>You do not want to buy their shops unknown in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13554</th>\n",
       "      <td>123</td>\n",
       "      <td>There�s something in store for you! Fist-rate ...</td>\n",
       "      <td>1</td>\n",
       "      <td>123 There�s something in store for you! Fist-r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57122</th>\n",
       "      <td>= ? iso - 8859 - 7 ? q ? = 5 b = 3 f = 5 d _ l...</td>\n",
       "      <td>premiere source for x : a : n : a : x , v : a ...</td>\n",
       "      <td>1</td>\n",
       "      <td>= ? iso - 8859 - 7 ? q ? = 5 b = 3 f = 5 d _ l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3378</th>\n",
       "      <td>Re: [Python-3000] Python 2.6 and 3.0</td>\n",
       "      <td>Barry Warsaw wrote:\\n&gt; From the follow ups, it...</td>\n",
       "      <td>0</td>\n",
       "      <td>Re: [Python-3000] Python 2.6 and 3.0 Barry War...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 subject  \\\n",
       "35921                        CNN Alerts: My Custom Alert   \n",
       "5507   You do not want to buy their shops unknown in ...   \n",
       "13554                                                123   \n",
       "57122  = ? iso - 8859 - 7 ? q ? = 5 b = 3 f = 5 d _ l...   \n",
       "3378                Re: [Python-3000] Python 2.6 and 3.0   \n",
       "\n",
       "                                                    body  label  \\\n",
       "35921  CNN Alerts: My Custom Alert\\n\\n\\n\\n\\n\\n\\n \\n\\n...      1   \n",
       "5507   Man's improving formula effective for for 90% ...      1   \n",
       "13554  There�s something in store for you! Fist-rate ...      1   \n",
       "57122  premiere source for x : a : n : a : x , v : a ...      1   \n",
       "3378   Barry Warsaw wrote:\\n> From the follow ups, it...      0   \n",
       "\n",
       "                                                    text  \n",
       "35921  CNN Alerts: My Custom Alert CNN Alerts: My Cus...  \n",
       "5507   You do not want to buy their shops unknown in ...  \n",
       "13554  123 There�s something in store for you! Fist-r...  \n",
       "57122  = ? iso - 8859 - 7 ? q ? = 5 b = 3 f = 5 d _ l...  \n",
       "3378   Re: [Python-3000] Python 2.6 and 3.0 Barry War...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df, val_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=df[\"label\"]\n",
    ")\n",
    "\n",
    "print(\"Train size:\", len(train_df))\n",
    "print(\"Val size:\", len(val_df))\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d47309c0-caa2-4240-b404-fc7aa9e86430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 201623\n",
      "Sample vocab items: [('<PAD>', 0), ('<UNK>', 1), ('-', 2), ('--', 3), ('---', 4), ('----', 5), ('-----', 6), ('------', 7), ('-------', 8), ('--------', 9), ('---------', 10), ('----------', 11), ('-----------', 12), ('------------', 13), ('-------------', 14), ('--------------', 15), ('---------------', 16), ('----------------', 17), ('-----------------', 18), ('------------------', 19)]\n"
     ]
    }
   ],
   "source": [
    "def simple_tokenize(text):\n",
    "    \"\"\"\n",
    "    Basic tokenizer: lowercase and split on whitespace after removing junk.\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z0-9@.\\-_/ ]+\", \" \", text)\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# Build vocab from training set only\n",
    "word_freq = {}\n",
    "for t in train_df[\"text\"]:\n",
    "    for tok in simple_tokenize(t):\n",
    "        word_freq[tok] = word_freq.get(tok, 0) + 1\n",
    "\n",
    "# Drop super-rare words\n",
    "min_freq = 2\n",
    "vocab_words = [w for w, f in word_freq.items() if f >= min_freq]\n",
    "\n",
    "# Reserve 0 for PAD, 1 for UNK\n",
    "itos = [\"<PAD>\", \"<UNK>\"] + sorted(vocab_words)\n",
    "stoi = {w: i for i, w in enumerate(itos)}\n",
    "\n",
    "vocab_size = len(itos)\n",
    "print(\"Vocab size:\", vocab_size)\n",
    "print(\"Sample vocab items:\", list(stoi.items())[:20])\n",
    "\n",
    "\n",
    "def numericalize(tokens, stoi_map, unk_idx=1):\n",
    "    return [stoi_map.get(tok, unk_idx) for tok in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d56fb7d-e64f-46c1-83e7-00b53ddf1119",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmailDataset(Dataset):\n",
    "    def __init__(self, df, stoi_map):\n",
    "        self.texts = df[\"text\"].tolist()\n",
    "        self.labels = df[\"label\"].tolist()\n",
    "        self.stoi_map = stoi_map\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        raw_text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        toks = simple_tokenize(raw_text)\n",
    "        ids = numericalize(toks, self.stoi_map)\n",
    "\n",
    "        return torch.tensor(ids, dtype=torch.long), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "\n",
    "def collate_batch(batch):\n",
    "    \"\"\"\n",
    "    Pad sequences to max length in batch.\n",
    "    \"\"\"\n",
    "    (seqs, labels) = zip(*batch)\n",
    "    padded_seqs = pad_sequence(seqs, batch_first=True, padding_value=0)\n",
    "    labels = torch.stack(labels)\n",
    "    return padded_seqs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "542d442a-0a00-4179-8bbf-c8e888e66054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DataLoader Configuration\n",
      "============================================================\n",
      "Batch size: 32\n",
      "Max sequence length: 512\n",
      "Train batches: 1909\n",
      "Val batches: 478\n",
      "\n",
      "Sample batch:\n",
      "  Batch X shape: torch.Size([32, 512])\n",
      "  Batch y shape: torch.Size([32])\n",
      "  Actual max length in batch: 512\n",
      "  First 50 token ids: tensor([ 55461,  62223, 184472,  12088, 182324,  62223, 184472,  12088,  89813,\n",
      "         55461, 184472, 192530,  37054, 176616,  39482, 142868,  40710,  11582,\n",
      "         16331,  19805,  24900, 151360,  74748, 184472,  12088, 192530,  11711,\n",
      "        135273, 135251, 103654,   5116, 190104,   8212, 110453, 110898,  31193,\n",
      "         67053,  70310, 110453, 110898,  31193, 187014, 110453, 110898, 182324,\n",
      "        135273, 135251,  55459, 165245, 113438])\n",
      "  First label: 1\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQ_LENGTH = 512  # Maximum tokens per email (adjust if needed)\n",
    "\n",
    "class EmailDataset(Dataset):\n",
    "    def __init__(self, df, stoi_map, max_length=512):\n",
    "        self.texts = df[\"text\"].tolist()\n",
    "        self.labels = df[\"label\"].tolist()\n",
    "        self.stoi_map = stoi_map\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        raw_text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        toks = simple_tokenize(raw_text)\n",
    "        \n",
    "        # TRUNCATE to max_length to prevent OOM\n",
    "        if len(toks) > self.max_length:\n",
    "            toks = toks[:self.max_length]\n",
    "        \n",
    "        ids = numericalize(toks, self.stoi_map)\n",
    "\n",
    "        return torch.tensor(ids, dtype=torch.long), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "\n",
    "def collate_batch(batch):\n",
    "    \"\"\"\n",
    "    Pad sequences to max length in batch.\n",
    "    \"\"\"\n",
    "    (seqs, labels) = zip(*batch)\n",
    "    padded_seqs = pad_sequence(seqs, batch_first=True, padding_value=0)\n",
    "    labels = torch.stack(labels)\n",
    "    return padded_seqs, labels\n",
    "\n",
    "\n",
    "# Create datasets with max_length limit\n",
    "train_dataset = EmailDataset(train_df, stoi, max_length=MAX_SEQ_LENGTH)\n",
    "val_dataset = EmailDataset(val_df, stoi, max_length=MAX_SEQ_LENGTH)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_batch\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_batch\n",
    ")\n",
    "\n",
    "# Sanity check\n",
    "batch_x, batch_y = next(iter(train_loader))\n",
    "print(\"=\"*60)\n",
    "print(\"DataLoader Configuration\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Max sequence length: {MAX_SEQ_LENGTH}\")\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")\n",
    "print(f\"\\nSample batch:\")\n",
    "print(f\"  Batch X shape: {batch_x.shape}\")  # Should be [32, <=512]\n",
    "print(f\"  Batch y shape: {batch_y.shape}\")  # Should be [32]\n",
    "print(f\"  Actual max length in batch: {batch_x.shape[1]}\")\n",
    "print(f\"  First 50 token ids: {batch_x[0][:50]}\")\n",
    "print(f\"  First label: {batch_y[0]}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32655eb8-63e4-4b2e-9dd3-b33bf1a74593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomCNN_Text(\n",
      "  (embedding): Embedding(201623, 128, padding_idx=0)\n",
      "  (conv1): Conv1d(128, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu1): ReLU(inplace=True)\n",
      "  (pool1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu2): ReLU(inplace=True)\n",
      "  (pool2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv1d(128, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (bn3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu3): ReLU(inplace=True)\n",
      "  (pool3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv4): Conv1d(256, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (bn4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu4): ReLU(inplace=True)\n",
      "  (pool4): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (global_pool): AdaptiveAvgPool1d(output_size=1)\n",
      "  (fc1): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (relu_fc1): ReLU(inplace=True)\n",
      "  (dropout1): Dropout(p=0.5, inplace=False)\n",
      "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (relu_fc2): ReLU(inplace=True)\n",
      "  (dropout2): Dropout(p=0.5, inplace=False)\n",
      "  (fc3): Linear(in_features=128, out_features=2, bias=True)\n",
      ")\n",
      "\n",
      "Total parameters: 26,515,778\n",
      "Trainable parameters: 26,515,778\n"
     ]
    }
   ],
   "source": [
    "class CustomCNN_Text(nn.Module):\n",
    "    \"\"\"\n",
    "    Custom CNN for text classification with medium depth.\n",
    "    Architecture: Embedding + 4 Conv1d blocks + 3 FC layers\n",
    "    MATCHES the structure of CustomCNN_Image but with Conv1d instead of Conv2d\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embed_dim=128, num_classes=2, dropout=0.5):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        \n",
    "        # Block 1: embed_dim -> 64 channels\n",
    "        self.conv1 = nn.Conv1d(embed_dim, 64, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Block 2: 64 -> 128 channels\n",
    "        self.conv2 = nn.Conv1d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Block 3: 128 -> 256 channels\n",
    "        self.conv3 = nn.Conv1d(128, 256, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm1d(256)\n",
    "        self.relu3 = nn.ReLU(inplace=True)\n",
    "        self.pool3 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Block 4: 256 -> 512 channels\n",
    "        self.conv4 = nn.Conv1d(256, 512, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm1d(512)\n",
    "        self.relu4 = nn.ReLU(inplace=True)\n",
    "        self.pool4 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Global Average Pooling (adaptive pooling to fixed size)\n",
    "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        \n",
    "        # Fully Connected Layers (SAME as image model)\n",
    "        self.fc1 = nn.Linear(512, 256)\n",
    "        self.relu_fc1 = nn.ReLU(inplace=True)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        \n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.relu_fc2 = nn.ReLU(inplace=True)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        \n",
    "        self.fc3 = nn.Linear(128, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Embedding: [B, T] -> [B, T, E]\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # Transpose for Conv1d: [B, T, E] -> [B, E, T]\n",
    "        x = x.transpose(1, 2)\n",
    "        \n",
    "        # Block 1\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        # Block 2\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        # Block 3\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.pool3(x)\n",
    "        \n",
    "        # Block 4\n",
    "        x = self.conv4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.pool4(x)\n",
    "        \n",
    "        # Global pooling: [B, 512, T] -> [B, 512, 1]\n",
    "        x = self.global_pool(x)\n",
    "        x = x.squeeze(-1)  # [B, 512]\n",
    "        \n",
    "        # FC layers\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu_fc1(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = self.relu_fc2(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "# Initialize model\n",
    "num_classes = df[\"label\"].nunique()\n",
    "model = CustomCNN_Text(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_dim=128,\n",
    "    num_classes=num_classes,\n",
    "    dropout=0.5\n",
    ").to(device)\n",
    "\n",
    "print(model)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5833bf02-1f68-4701-bdfe-d86014b4fdae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Criterion: CrossEntropyLoss()\n",
      "Optimizer: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "print(\"Criterion:\", criterion)\n",
    "print(\"Optimizer:\", optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc623a2b-20d4-4de2-a9b6-ecfb65f67832",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_count = 0\n",
    "\n",
    "    for batch_x, batch_y in tqdm(loader, desc=\"Train\", leave=False):\n",
    "        batch_x = batch_x.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits = model(batch_x)\n",
    "        loss = criterion(logits, batch_y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * batch_x.size(0)\n",
    "\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        total_correct += (preds == batch_y).sum().item()\n",
    "        total_count += batch_x.size(0)\n",
    "\n",
    "    avg_loss = total_loss / total_count\n",
    "    avg_acc = total_correct / total_count\n",
    "    return avg_loss, avg_acc\n",
    "\n",
    "\n",
    "def eval_one_epoch(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in tqdm(loader, desc=\"Val\", leave=False):\n",
    "            batch_x = batch_x.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "\n",
    "            logits = model(batch_x)\n",
    "            loss = criterion(logits, batch_y)\n",
    "\n",
    "            total_loss += loss.item() * batch_x.size(0)\n",
    "\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            total_correct += (preds == batch_y).sum().item()\n",
    "            total_count += batch_x.size(0)\n",
    "\n",
    "    avg_loss = total_loss / total_count\n",
    "    avg_acc = total_correct / total_count\n",
    "    return avg_loss, avg_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "324d7a5d-5757-41cd-aebd-806c91608709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "GPU Memory Status\n",
      "============================================================\n",
      "Device: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "Total Memory: 8.59 GB\n",
      "Allocated: 0.11 GB\n",
      "Cached: 0.11 GB\n",
      "Available: 8.48 GB\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Clear GPU memory before training\n",
    "import gc\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"GPU Memory Status\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Device: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"Total Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "print(f\"Allocated: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")\n",
    "print(f\"Cached: {torch.cuda.memory_reserved(0) / 1e9:.2f} GB\")\n",
    "print(f\"Available: {(torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated(0)) / 1e9:.2f} GB\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab01e46f-f9ee-42f4-ba58-0df3693c454c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for 10 epochs...\n",
      "Total training batches: 1909\n",
      "Total validation batches: 478\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "Epoch 1/10\n",
      "======================================================================\n",
      "[TRAINING] Processing 1909 batches...\n",
      "  [ 10.0%] Batch  190/1909 | Loss: 0.1877 | Acc: 0.8447\n",
      "  [ 19.9%] Batch  380/1909 | Loss: 0.1261 | Acc: 0.8918\n",
      "  [ 29.9%] Batch  570/1909 | Loss: 0.1986 | Acc: 0.9113\n",
      "  [ 39.8%] Batch  760/1909 | Loss: 0.5213 | Acc: 0.9240\n",
      "  [ 49.8%] Batch  950/1909 | Loss: 0.0385 | Acc: 0.9322\n",
      "  [ 59.7%] Batch 1140/1909 | Loss: 0.1866 | Acc: 0.9379\n",
      "  [ 69.7%] Batch 1330/1909 | Loss: 0.1272 | Acc: 0.9417\n",
      "  [ 79.6%] Batch 1520/1909 | Loss: 0.0075 | Acc: 0.9455\n",
      "  [ 89.6%] Batch 1710/1909 | Loss: 0.0192 | Acc: 0.9490\n",
      "  [ 99.5%] Batch 1900/1909 | Loss: 0.0033 | Acc: 0.9522\n",
      "  [100.0%] Batch 1909/1909 | Loss: 0.4040 | Acc: 0.9523\n",
      "\n",
      "[VALIDATION] Processing 478 batches...\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "[Epoch 1/10 Summary]\n",
      "  Train Loss: 0.1363 | Train Acc: 0.9523 (95.23%)\n",
      "  Val Loss:   0.0722   | Val Acc:   0.9775 (97.75%)\n",
      "  New BEST model! (val_acc=0.9775)\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "======================================================================\n",
      "Epoch 2/10\n",
      "======================================================================\n",
      "[TRAINING] Processing 1909 batches...\n",
      "  [ 10.0%] Batch  190/1909 | Loss: 0.0602 | Acc: 0.9880\n",
      "  [ 19.9%] Batch  380/1909 | Loss: 0.0055 | Acc: 0.9887\n",
      "  [ 29.9%] Batch  570/1909 | Loss: 0.0136 | Acc: 0.9880\n",
      "  [ 39.8%] Batch  760/1909 | Loss: 0.0177 | Acc: 0.9878\n",
      "  [ 49.8%] Batch  950/1909 | Loss: 0.0006 | Acc: 0.9883\n",
      "  [ 59.7%] Batch 1140/1909 | Loss: 0.0389 | Acc: 0.9877\n",
      "  [ 69.7%] Batch 1330/1909 | Loss: 0.0051 | Acc: 0.9882\n",
      "  [ 79.6%] Batch 1520/1909 | Loss: 0.0099 | Acc: 0.9885\n",
      "  [ 89.6%] Batch 1710/1909 | Loss: 0.0065 | Acc: 0.9887\n",
      "  [ 99.5%] Batch 1900/1909 | Loss: 0.0105 | Acc: 0.9889\n",
      "  [100.0%] Batch 1909/1909 | Loss: 0.1756 | Acc: 0.9888\n",
      "\n",
      "[VALIDATION] Processing 478 batches...\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "[Epoch 2/10 Summary]\n",
      "  Train Loss: 0.0362 | Train Acc: 0.9888 (98.88%)\n",
      "  Val Loss:   0.0729   | Val Acc:   0.9807 (98.07%)\n",
      "  New BEST model! (val_acc=0.9807)\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "======================================================================\n",
      "Epoch 3/10\n",
      "======================================================================\n",
      "[TRAINING] Processing 1909 batches...\n",
      "  [ 10.0%] Batch  190/1909 | Loss: 0.0066 | Acc: 0.9954\n",
      "  [ 19.9%] Batch  380/1909 | Loss: 0.0263 | Acc: 0.9947\n",
      "  [ 29.9%] Batch  570/1909 | Loss: 0.0028 | Acc: 0.9949\n",
      "  [ 39.8%] Batch  760/1909 | Loss: 0.0066 | Acc: 0.9946\n",
      "  [ 49.8%] Batch  950/1909 | Loss: 0.0185 | Acc: 0.9945\n",
      "  [ 59.7%] Batch 1140/1909 | Loss: 0.0014 | Acc: 0.9944\n",
      "  [ 69.7%] Batch 1330/1909 | Loss: 0.0023 | Acc: 0.9945\n",
      "  [ 79.6%] Batch 1520/1909 | Loss: 0.0002 | Acc: 0.9944\n",
      "  [ 89.6%] Batch 1710/1909 | Loss: 0.0007 | Acc: 0.9946\n",
      "  [ 99.5%] Batch 1900/1909 | Loss: 0.0084 | Acc: 0.9947\n",
      "  [100.0%] Batch 1909/1909 | Loss: 0.0000 | Acc: 0.9947\n",
      "\n",
      "[VALIDATION] Processing 478 batches...\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "[Epoch 3/10 Summary]\n",
      "  Train Loss: 0.0186 | Train Acc: 0.9947 (99.47%)\n",
      "  Val Loss:   0.0405   | Val Acc:   0.9895 (98.95%)\n",
      "  New BEST model! (val_acc=0.9895)\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "======================================================================\n",
      "Epoch 4/10\n",
      "======================================================================\n",
      "[TRAINING] Processing 1909 batches...\n",
      "  [ 10.0%] Batch  190/1909 | Loss: 0.0001 | Acc: 0.9962\n",
      "  [ 19.9%] Batch  380/1909 | Loss: 0.0007 | Acc: 0.9973\n",
      "  [ 29.9%] Batch  570/1909 | Loss: 0.0012 | Acc: 0.9969\n",
      "  [ 39.8%] Batch  760/1909 | Loss: 0.0004 | Acc: 0.9969\n",
      "  [ 49.8%] Batch  950/1909 | Loss: 0.0321 | Acc: 0.9968\n",
      "  [ 59.7%] Batch 1140/1909 | Loss: 0.0119 | Acc: 0.9968\n",
      "  [ 69.7%] Batch 1330/1909 | Loss: 0.0001 | Acc: 0.9968\n",
      "  [ 79.6%] Batch 1520/1909 | Loss: 0.0017 | Acc: 0.9966\n",
      "  [ 89.6%] Batch 1710/1909 | Loss: 0.0058 | Acc: 0.9966\n",
      "  [ 99.5%] Batch 1900/1909 | Loss: 0.0006 | Acc: 0.9961\n",
      "  [100.0%] Batch 1909/1909 | Loss: 0.0665 | Acc: 0.9961\n",
      "\n",
      "[VALIDATION] Processing 478 batches...\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "[Epoch 4/10 Summary]\n",
      "  Train Loss: 0.0128 | Train Acc: 0.9961 (99.61%)\n",
      "  Val Loss:   0.0682   | Val Acc:   0.9866 (98.66%)\n",
      "  (Best val_acc so far: 0.9895)\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "======================================================================\n",
      "Epoch 5/10\n",
      "======================================================================\n",
      "[TRAINING] Processing 1909 batches...\n",
      "  [ 10.0%] Batch  190/1909 | Loss: 0.0001 | Acc: 0.9975\n",
      "  [ 19.9%] Batch  380/1909 | Loss: 0.0001 | Acc: 0.9975\n",
      "  [ 29.9%] Batch  570/1909 | Loss: 0.0012 | Acc: 0.9976\n",
      "  [ 39.8%] Batch  760/1909 | Loss: 0.0001 | Acc: 0.9977\n",
      "  [ 49.8%] Batch  950/1909 | Loss: 0.0052 | Acc: 0.9978\n",
      "  [ 59.7%] Batch 1140/1909 | Loss: 0.0004 | Acc: 0.9976\n",
      "  [ 69.7%] Batch 1330/1909 | Loss: 0.0001 | Acc: 0.9978\n",
      "  [ 79.6%] Batch 1520/1909 | Loss: 0.0003 | Acc: 0.9978\n",
      "  [ 89.6%] Batch 1710/1909 | Loss: 0.0005 | Acc: 0.9979\n",
      "  [ 99.5%] Batch 1900/1909 | Loss: 0.0002 | Acc: 0.9979\n",
      "  [100.0%] Batch 1909/1909 | Loss: 0.0019 | Acc: 0.9979\n",
      "\n",
      "[VALIDATION] Processing 478 batches...\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "[Epoch 5/10 Summary]\n",
      "  Train Loss: 0.0076 | Train Acc: 0.9979 (99.79%)\n",
      "  Val Loss:   0.0615   | Val Acc:   0.9861 (98.61%)\n",
      "  (Best val_acc so far: 0.9895)\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "======================================================================\n",
      "Epoch 6/10\n",
      "======================================================================\n",
      "[TRAINING] Processing 1909 batches...\n",
      "  [ 10.0%] Batch  190/1909 | Loss: 0.0004 | Acc: 0.9969\n",
      "  [ 19.9%] Batch  380/1909 | Loss: 0.0206 | Acc: 0.9970\n",
      "  [ 29.9%] Batch  570/1909 | Loss: 0.0013 | Acc: 0.9976\n",
      "  [ 39.8%] Batch  760/1909 | Loss: 0.0000 | Acc: 0.9977\n",
      "  [ 49.8%] Batch  950/1909 | Loss: 0.0000 | Acc: 0.9980\n",
      "  [ 59.7%] Batch 1140/1909 | Loss: 0.0033 | Acc: 0.9979\n",
      "  [ 69.7%] Batch 1330/1909 | Loss: 0.0003 | Acc: 0.9980\n",
      "  [ 79.6%] Batch 1520/1909 | Loss: 0.0007 | Acc: 0.9982\n",
      "  [ 89.6%] Batch 1710/1909 | Loss: 0.0000 | Acc: 0.9983\n",
      "  [ 99.5%] Batch 1900/1909 | Loss: 0.0002 | Acc: 0.9984\n",
      "  [100.0%] Batch 1909/1909 | Loss: 0.0001 | Acc: 0.9984\n",
      "\n",
      "[VALIDATION] Processing 478 batches...\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "[Epoch 6/10 Summary]\n",
      "  Train Loss: 0.0061 | Train Acc: 0.9984 (99.84%)\n",
      "  Val Loss:   0.0485   | Val Acc:   0.9903 (99.03%)\n",
      "  New BEST model! (val_acc=0.9903)\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "======================================================================\n",
      "Epoch 7/10\n",
      "======================================================================\n",
      "[TRAINING] Processing 1909 batches...\n",
      "  [ 10.0%] Batch  190/1909 | Loss: 0.0000 | Acc: 0.9998\n",
      "  [ 19.9%] Batch  380/1909 | Loss: 0.0001 | Acc: 0.9998\n",
      "  [ 29.9%] Batch  570/1909 | Loss: 0.0001 | Acc: 0.9994\n",
      "  [ 39.8%] Batch  760/1909 | Loss: 0.0010 | Acc: 0.9995\n",
      "  [ 49.8%] Batch  950/1909 | Loss: 0.0000 | Acc: 0.9992\n",
      "  [ 59.7%] Batch 1140/1909 | Loss: 0.0000 | Acc: 0.9990\n",
      "  [ 69.7%] Batch 1330/1909 | Loss: 0.0008 | Acc: 0.9984\n",
      "  [ 79.6%] Batch 1520/1909 | Loss: 0.1148 | Acc: 0.9983\n",
      "  [ 89.6%] Batch 1710/1909 | Loss: 0.0000 | Acc: 0.9983\n",
      "  [ 99.5%] Batch 1900/1909 | Loss: 0.0153 | Acc: 0.9983\n",
      "  [100.0%] Batch 1909/1909 | Loss: 0.0000 | Acc: 0.9983\n",
      "\n",
      "[VALIDATION] Processing 478 batches...\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "[Epoch 7/10 Summary]\n",
      "  Train Loss: 0.0085 | Train Acc: 0.9983 (99.83%)\n",
      "  Val Loss:   0.0749   | Val Acc:   0.9878 (98.78%)\n",
      "  (Best val_acc so far: 0.9903)\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "======================================================================\n",
      "Epoch 8/10\n",
      "======================================================================\n",
      "[TRAINING] Processing 1909 batches...\n",
      "  [ 10.0%] Batch  190/1909 | Loss: 0.0002 | Acc: 0.9990\n",
      "  [ 19.9%] Batch  380/1909 | Loss: 0.0000 | Acc: 0.9992\n",
      "  [ 29.9%] Batch  570/1909 | Loss: 0.0001 | Acc: 0.9989\n",
      "  [ 39.8%] Batch  760/1909 | Loss: 0.0000 | Acc: 0.9991\n",
      "  [ 49.8%] Batch  950/1909 | Loss: 0.0000 | Acc: 0.9991\n",
      "  [ 59.7%] Batch 1140/1909 | Loss: 0.0000 | Acc: 0.9990\n",
      "  [ 69.7%] Batch 1330/1909 | Loss: 0.0002 | Acc: 0.9990\n",
      "  [ 79.6%] Batch 1520/1909 | Loss: 0.0000 | Acc: 0.9989\n",
      "  [ 89.6%] Batch 1710/1909 | Loss: 0.0000 | Acc: 0.9989\n",
      "  [ 99.5%] Batch 1900/1909 | Loss: 0.0003 | Acc: 0.9989\n",
      "  [100.0%] Batch 1909/1909 | Loss: 0.0067 | Acc: 0.9989\n",
      "\n",
      "[VALIDATION] Processing 478 batches...\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "[Epoch 8/10 Summary]\n",
      "  Train Loss: 0.0044 | Train Acc: 0.9989 (99.89%)\n",
      "  Val Loss:   0.0689   | Val Acc:   0.9876 (98.76%)\n",
      "  (Best val_acc so far: 0.9903)\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "======================================================================\n",
      "Epoch 9/10\n",
      "======================================================================\n",
      "[TRAINING] Processing 1909 batches...\n",
      "  [ 10.0%] Batch  190/1909 | Loss: 0.0001 | Acc: 0.9997\n",
      "  [ 19.9%] Batch  380/1909 | Loss: 0.0000 | Acc: 0.9993\n",
      "  [ 29.9%] Batch  570/1909 | Loss: 0.0000 | Acc: 0.9995\n",
      "  [ 39.8%] Batch  760/1909 | Loss: 0.0009 | Acc: 0.9995\n",
      "  [ 49.8%] Batch  950/1909 | Loss: 0.0001 | Acc: 0.9993\n",
      "  [ 59.7%] Batch 1140/1909 | Loss: 0.0001 | Acc: 0.9993\n",
      "  [ 69.7%] Batch 1330/1909 | Loss: 0.0000 | Acc: 0.9993\n",
      "  [ 79.6%] Batch 1520/1909 | Loss: 0.0000 | Acc: 0.9993\n",
      "  [ 89.6%] Batch 1710/1909 | Loss: 0.0003 | Acc: 0.9993\n",
      "  [ 99.5%] Batch 1900/1909 | Loss: 0.0000 | Acc: 0.9993\n",
      "  [100.0%] Batch 1909/1909 | Loss: 0.0000 | Acc: 0.9993\n",
      "\n",
      "[VALIDATION] Processing 478 batches...\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "[Epoch 9/10 Summary]\n",
      "  Train Loss: 0.0034 | Train Acc: 0.9993 (99.93%)\n",
      "  Val Loss:   0.0772   | Val Acc:   0.9900 (99.00%)\n",
      "  (Best val_acc so far: 0.9903)\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "======================================================================\n",
      "Epoch 10/10\n",
      "======================================================================\n",
      "[TRAINING] Processing 1909 batches...\n",
      "  [ 10.0%] Batch  190/1909 | Loss: 0.0000 | Acc: 1.0000\n",
      "  [ 19.9%] Batch  380/1909 | Loss: 0.0009 | Acc: 0.9989\n",
      "  [ 29.9%] Batch  570/1909 | Loss: 0.0009 | Acc: 0.9989\n",
      "  [ 39.8%] Batch  760/1909 | Loss: 0.0000 | Acc: 0.9988\n",
      "  [ 49.8%] Batch  950/1909 | Loss: 0.0000 | Acc: 0.9989\n",
      "  [ 59.7%] Batch 1140/1909 | Loss: 0.0000 | Acc: 0.9991\n",
      "  [ 69.7%] Batch 1330/1909 | Loss: 0.0001 | Acc: 0.9991\n",
      "  [ 79.6%] Batch 1520/1909 | Loss: 0.0000 | Acc: 0.9991\n",
      "  [ 89.6%] Batch 1710/1909 | Loss: 0.0000 | Acc: 0.9990\n",
      "  [ 99.5%] Batch 1900/1909 | Loss: 0.0000 | Acc: 0.9990\n",
      "  [100.0%] Batch 1909/1909 | Loss: 0.0008 | Acc: 0.9990\n",
      "\n",
      "[VALIDATION] Processing 478 batches...\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "[Epoch 10/10 Summary]\n",
      "  Train Loss: 0.0048 | Train Acc: 0.9990 (99.90%)\n",
      "  Val Loss:   0.0699   | Val Acc:   0.9887 (98.87%)\n",
      "  (Best val_acc so far: 0.9903)\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "======================================================================\n",
      "=== Training Complete ===\n",
      "======================================================================\n",
      "Best Validation Accuracy: 0.9903 (99.03%)\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "best_val_acc = 0.0\n",
    "best_state_dict = None\n",
    "\n",
    "history = {\n",
    "    \"train_loss\": [],\n",
    "    \"train_acc\": [],\n",
    "    \"val_loss\": [],\n",
    "    \"val_acc\": []\n",
    "}\n",
    "\n",
    "print(f\"Starting training for {EPOCHS} epochs...\")\n",
    "print(f\"Total training batches: {len(train_loader)}\")\n",
    "print(f\"Total validation batches: {len(val_loader)}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # ============================================\n",
    "    # TRAINING PHASE\n",
    "    # ============================================\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_correct = 0\n",
    "    running_total = 0\n",
    "    \n",
    "    total_train_batches = len(train_loader)\n",
    "    print_every = max(1, total_train_batches // 10)  # Print 10 times per epoch\n",
    "    \n",
    "    print(f\"[TRAINING] Processing {total_train_batches} batches...\")\n",
    "    \n",
    "    for batch_idx, (batch_x, batch_y) in enumerate(train_loader):\n",
    "        batch_x = batch_x.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits = model(batch_x)\n",
    "        loss = criterion(logits, batch_y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * batch_x.size(0)\n",
    "\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        running_correct += (preds == batch_y).sum().item()\n",
    "        running_total += batch_x.size(0)\n",
    "        \n",
    "        # Print progress\n",
    "        if (batch_idx + 1) % print_every == 0 or (batch_idx + 1) == total_train_batches:\n",
    "            current_acc = running_correct / running_total\n",
    "            progress = (batch_idx + 1) / total_train_batches * 100\n",
    "            print(f\"  [{progress:5.1f}%] Batch {batch_idx+1:4d}/{total_train_batches} | \"\n",
    "                  f\"Loss: {loss.item():.4f} | Acc: {current_acc:.4f}\")\n",
    "\n",
    "    train_loss = running_loss / running_total\n",
    "    train_acc = running_correct / running_total\n",
    "    \n",
    "    # ============================================\n",
    "    # VALIDATION PHASE\n",
    "    # ============================================\n",
    "    print(f\"\\n[VALIDATION] Processing {len(val_loader)} batches...\")\n",
    "    \n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_correct = 0\n",
    "    running_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (batch_x, batch_y) in enumerate(val_loader):\n",
    "            batch_x = batch_x.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "\n",
    "            logits = model(batch_x)\n",
    "            loss = criterion(logits, batch_y)\n",
    "\n",
    "            running_loss += loss.item() * batch_x.size(0)\n",
    "\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            running_correct += (preds == batch_y).sum().item()\n",
    "            running_total += batch_x.size(0)\n",
    "    \n",
    "    val_loss = running_loss / running_total\n",
    "    val_acc = running_correct / running_total\n",
    "    \n",
    "    # ============================================\n",
    "    # SAVE METRICS & PRINT SUMMARY\n",
    "    # ============================================\n",
    "    history[\"train_loss\"].append(train_loss)\n",
    "    history[\"train_acc\"].append(train_acc)\n",
    "    history[\"val_loss\"].append(val_loss)\n",
    "    history[\"val_acc\"].append(val_acc)\n",
    "    \n",
    "    print(f\"\\n{'-'*70}\")\n",
    "    print(f\"[Epoch {epoch+1}/{EPOCHS} Summary]\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} ({train_acc*100:.2f}%)\")\n",
    "    print(f\"  Val Loss:   {val_loss:.4f}   | Val Acc:   {val_acc:.4f} ({val_acc*100:.2f}%)\")\n",
    "    \n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_state_dict = {k: v.cpu() for k, v in model.state_dict().items()}\n",
    "        print(f\"  New BEST model! (val_acc={best_val_acc:.4f})\")\n",
    "    else:\n",
    "        print(f\"  (Best val_acc so far: {best_val_acc:.4f})\")\n",
    "    \n",
    "    print(f\"{'-'*70}\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"=== Training Complete ===\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Best Validation Accuracy: {best_val_acc:.4f} ({best_val_acc*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c50d9f5-c6da-4ecf-8c1d-8605ee79253b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved best model -> best_custom_cnn_text_2.pth\n",
      "Saved vocab_text_2.json\n",
      "\n",
      "Training curves:\n",
      "Epoch 01: train_acc=0.9523, val_acc=0.9775, train_loss=0.1363, val_loss=0.0722\n",
      "Epoch 02: train_acc=0.9888, val_acc=0.9807, train_loss=0.0362, val_loss=0.0729\n",
      "Epoch 03: train_acc=0.9947, val_acc=0.9895, train_loss=0.0186, val_loss=0.0405\n",
      "Epoch 04: train_acc=0.9961, val_acc=0.9866, train_loss=0.0128, val_loss=0.0682\n",
      "Epoch 05: train_acc=0.9979, val_acc=0.9861, train_loss=0.0076, val_loss=0.0615\n",
      "Epoch 06: train_acc=0.9984, val_acc=0.9903, train_loss=0.0061, val_loss=0.0485\n",
      "Epoch 07: train_acc=0.9983, val_acc=0.9878, train_loss=0.0085, val_loss=0.0749\n",
      "Epoch 08: train_acc=0.9989, val_acc=0.9876, train_loss=0.0044, val_loss=0.0689\n",
      "Epoch 09: train_acc=0.9993, val_acc=0.9900, train_loss=0.0034, val_loss=0.0772\n",
      "Epoch 10: train_acc=0.9990, val_acc=0.9887, train_loss=0.0048, val_loss=0.0699\n"
     ]
    }
   ],
   "source": [
    "if best_state_dict is not None:\n",
    "    torch.save(best_state_dict, \"best_custom_cnn_text_2.pth\")\n",
    "    print(\"Saved best model -> best_custom_cnn_text_2.pth\")\n",
    "\n",
    "with open(\"vocab_text_2.json\", \"w\") as f:\n",
    "    json.dump({\"itos\": itos}, f, indent=2)\n",
    "print(\"Saved vocab_text_2.json\")\n",
    "\n",
    "print(\"\\nTraining curves:\")\n",
    "for i in range(EPOCHS):\n",
    "    print(\n",
    "        f\"Epoch {i+1:02d}: \"\n",
    "        f\"train_acc={history['train_acc'][i]:.4f}, \"\n",
    "        f\"val_acc={history['val_acc'][i]:.4f}, \"\n",
    "        f\"train_loss={history['train_loss'][i]:.4f}, \"\n",
    "        f\"val_loss={history['val_loss'][i]:.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b47dcd6-bc68-4581-95c6-c555fc459ccb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (GPU)",
   "language": "python",
   "name": "my_gpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
