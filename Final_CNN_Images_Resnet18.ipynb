{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c40db48-bc8d-4e9c-975e-562b6ea1d732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in c:\\users\\srihari\\anaconda3\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\srihari\\anaconda3\\lib\\site-packages (from tqdm) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a7ab48e-5b1a-4593-87d4-ef4bc3251d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "CUDA Available: True\n",
      "Device Count: 1\n",
      "Current Device: 0\n",
      "Device Name: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "Device Capability: (8, 9)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from glob import glob\n",
    "import xml.etree.ElementTree as ET\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "import numpy as np\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'  # Force GPU 0 (NVIDIA)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"Device Count:\", torch.cuda.device_count())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Current Device:\", torch.cuda.current_device())\n",
    "    print(\"Device Name:\", torch.cuda.get_device_name(0))\n",
    "    print(\"Device Capability:\", torch.cuda.get_device_capability(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94856d52-c955-485d-b915-2665887175bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE_DIR:      C:\\Users\\Srihari\\ENPM703FinalProject\n",
      "OPENLOGO_DIR:  C:\\Users\\Srihari\\ENPM703FinalProject\\openlogo\n",
      "JPEG_DIR:      C:\\Users\\Srihari\\ENPM703FinalProject\\openlogo\\JPEGImages\n",
      "ANNO_DIR:      C:\\Users\\Srihari\\ENPM703FinalProject\\openlogo\\Annotations\n",
      "IMAGESET_DIR:  C:\\Users\\Srihari\\ENPM703FinalProject\\openlogo\\ImageSets\\class_sep\n",
      "\n",
      "Sample images: ['C:\\\\Users\\\\Srihari\\\\ENPM703FinalProject\\\\openlogo\\\\JPEGImages\\\\1008198576.jpg', 'C:\\\\Users\\\\Srihari\\\\ENPM703FinalProject\\\\openlogo\\\\JPEGImages\\\\1016381746.jpg']\n",
      "Sample annos:  ['C:\\\\Users\\\\Srihari\\\\ENPM703FinalProject\\\\openlogo\\\\Annotations\\\\1008198576.xml', 'C:\\\\Users\\\\Srihari\\\\ENPM703FinalProject\\\\openlogo\\\\Annotations\\\\1016381746.xml']\n"
     ]
    }
   ],
   "source": [
    "# Path configuration\n",
    "BASE_DIR = os.path.abspath(\".\")\n",
    "OPENLOGO_DIR = os.path.join(BASE_DIR, \"openlogo\")\n",
    "\n",
    "JPEG_DIR = os.path.join(OPENLOGO_DIR, \"JPEGImages\")\n",
    "ANNO_DIR = os.path.join(OPENLOGO_DIR, \"Annotations\")\n",
    "IMAGESET_DIR = os.path.join(OPENLOGO_DIR, \"ImageSets\", \"class_sep\")\n",
    "\n",
    "print(\"BASE_DIR:     \", BASE_DIR)\n",
    "print(\"OPENLOGO_DIR: \", OPENLOGO_DIR)\n",
    "print(\"JPEG_DIR:     \", JPEG_DIR)\n",
    "print(\"ANNO_DIR:     \", ANNO_DIR)\n",
    "print(\"IMAGESET_DIR: \", IMAGESET_DIR)\n",
    "\n",
    "# Sanity check\n",
    "some_imgs = glob(os.path.join(JPEG_DIR, \"*.jpg\"))[:5]\n",
    "some_annos = glob(os.path.join(ANNO_DIR, \"*.xml\"))[:5]\n",
    "print(\"\\nSample images:\", some_imgs[:2])\n",
    "print(\"Sample annos: \", some_annos[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c5dd7a8-16da-40e6-9a36-d8d01692328b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing parse_voc_xml on: C:\\Users\\Srihari\\ENPM703FinalProject\\openlogo\\Annotations\\1008198576.xml\n",
      "[{'class_name': 'guinness', 'bbox': [333, 238, 595, 414]}]\n"
     ]
    }
   ],
   "source": [
    "def parse_voc_xml(xml_path):\n",
    "    \"\"\"\n",
    "    Parse one XML annotation. Return a list of objects.\n",
    "    Each object is: {'class_name': str, 'bbox': [xmin, ymin, xmax, ymax]}\n",
    "    \"\"\"\n",
    "    tree = ET.parse(xml_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    objects = []\n",
    "    for obj in root.findall(\"object\"):\n",
    "        cls_name = obj.find(\"name\").text.strip()\n",
    "\n",
    "        bbox_node = obj.find(\"bndbox\")\n",
    "        xmin = int(float(bbox_node.find(\"xmin\").text))\n",
    "        ymin = int(float(bbox_node.find(\"ymin\").text))\n",
    "        xmax = int(float(bbox_node.find(\"xmax\").text))\n",
    "        ymax = int(float(bbox_node.find(\"ymax\").text))\n",
    "\n",
    "        objects.append({\n",
    "            \"class_name\": cls_name,\n",
    "            \"bbox\": [xmin, ymin, xmax, ymax]\n",
    "        })\n",
    "\n",
    "    return objects\n",
    "\n",
    "# Test\n",
    "if len(some_annos) > 0:\n",
    "    test_xml = some_annos[0]\n",
    "    print(\"Testing parse_voc_xml on:\", test_xml)\n",
    "    print(parse_voc_xml(test_xml)[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d47309c0-caa2-4240-b404-fc7aa9e86430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found train list files: 353\n",
      "Discovered classes: 352\n",
      "Total samples: 72652 -> train 58121, val 14531\n",
      "num_classes: 352\n",
      "\n",
      "Sample: {'img_path': 'C:\\\\Users\\\\Srihari\\\\ENPM703FinalProject\\\\openlogo\\\\JPEGImages\\\\CPA_Australia_sportslogo_22.jpg', 'bbox': [1, 330, 54, 352], 'class_name': 'cpa_australia', 'label': 104}\n"
     ]
    }
   ],
   "source": [
    "def build_logo_samples(imageset_dir, jpeg_dir, anno_dir, \n",
    "                       train_suffix=\"_train.txt\", test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Build train/val samples from OpenLogo dataset.\n",
    "    Returns: train_samples, val_samples, class_to_idx\n",
    "    \"\"\"\n",
    "    all_samples = []\n",
    "\n",
    "    list_files = glob(os.path.join(imageset_dir, f\"*{train_suffix}\"))\n",
    "    list_files.sort()\n",
    "    print(\"Found train list files:\", len(list_files))\n",
    "\n",
    "    for lf in list_files:\n",
    "        with open(lf, \"r\") as f:\n",
    "            ids = [line.strip() for line in f if len(line.strip()) > 0]\n",
    "\n",
    "        for img_id in ids:\n",
    "            img_path = os.path.join(jpeg_dir, img_id + \".jpg\")\n",
    "            xml_path = os.path.join(anno_dir, img_id + \".xml\")\n",
    "\n",
    "            if not (os.path.exists(img_path) and os.path.exists(xml_path)):\n",
    "                continue\n",
    "\n",
    "            objs = parse_voc_xml(xml_path)\n",
    "\n",
    "            for obj in objs:\n",
    "                cls_name = obj[\"class_name\"]\n",
    "                bbox = obj[\"bbox\"]\n",
    "\n",
    "                all_samples.append({\n",
    "                    \"img_path\": img_path,\n",
    "                    \"bbox\": bbox,\n",
    "                    \"class_name\": cls_name\n",
    "                })\n",
    "\n",
    "    # Build class mapping\n",
    "    classes = sorted(list({s[\"class_name\"] for s in all_samples}))\n",
    "    class_to_idx = {c: i for i, c in enumerate(classes)}\n",
    "    print(\"Discovered classes:\", len(class_to_idx))\n",
    "\n",
    "    # Add numeric labels\n",
    "    for s in all_samples:\n",
    "        s[\"label\"] = class_to_idx[s[\"class_name\"]]\n",
    "\n",
    "    # Stratified split\n",
    "    y = [s[\"label\"] for s in all_samples]\n",
    "    train_s, val_s = train_test_split(\n",
    "        all_samples,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "        stratify=y\n",
    "    )\n",
    "\n",
    "    print(f\"Total samples: {len(all_samples)} -> train {len(train_s)}, val {len(val_s)}\")\n",
    "    return train_s, val_s, class_to_idx\n",
    "\n",
    "\n",
    "train_samples, val_samples, class_to_idx = build_logo_samples(\n",
    "    IMAGESET_DIR, JPEG_DIR, ANNO_DIR,\n",
    "    train_suffix=\"_train.txt\", test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "num_classes = len(class_to_idx)\n",
    "print(\"num_classes:\", num_classes)\n",
    "print(\"\\nSample:\", train_samples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d56fb7d-e64f-46c1-83e7-00b53ddf1119",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogoCropDataset(Dataset):\n",
    "    def __init__(self, samples, transform=None):\n",
    "        self.samples = samples\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        s = self.samples[idx]\n",
    "        path = s[\"img_path\"]\n",
    "        xmin, ymin, xmax, ymax = s[\"bbox\"]\n",
    "        label = s[\"label\"]\n",
    "\n",
    "        # Load image\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "\n",
    "        # Defensive clamp\n",
    "        w, h = img.size\n",
    "        xmin_c = max(0, min(xmin, w - 1))\n",
    "        ymin_c = max(0, min(ymin, h - 1))\n",
    "        xmax_c = max(0, min(xmax, w))\n",
    "        ymax_c = max(0, min(ymax, h))\n",
    "\n",
    "        if xmax_c <= xmin_c or ymax_c <= ymin_c:\n",
    "            crop = img\n",
    "        else:\n",
    "            crop = img.crop((xmin_c, ymin_c, xmax_c, ymax_c))\n",
    "\n",
    "        if self.transform is not None:\n",
    "            crop = self.transform(crop)\n",
    "\n",
    "        return crop, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "542d442a-0a00-4179-8bbf-c8e888e66054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 1817 Val batches: 455\n"
     ]
    }
   ],
   "source": [
    "train_transform = T.Compose([\n",
    "    T.Resize((224, 224)),\n",
    "    T.RandomHorizontalFlip(p=0.5),\n",
    "    T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.05),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "val_transform = T.Compose([\n",
    "    T.Resize((224, 224)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "train_ds = LogoCropDataset(train_samples, transform=train_transform)\n",
    "val_ds = LogoCropDataset(val_samples, transform=val_transform)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=(device.type == \"cuda\")\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=(device.type == \"cuda\")\n",
    ")\n",
    "\n",
    "print(\"Train batches:\", len(train_loader), \"Val batches:\", len(val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32655eb8-63e4-4b2e-9dd3-b33bf1a74593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total parameters: 11,357,088\n",
      "Trainable parameters: 11,357,088\n",
      "\n",
      "Model ready!\n"
     ]
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "# Use pretrained ResNet18\n",
    "model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "\n",
    "# Replace final layer for 352 classes\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(\"\\nModel ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5833bf02-1f68-4701-bdfe-d86014b4fdae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Criterion: CrossEntropyLoss()\n",
      "Optimizer: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.0001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "print(\"Criterion:\", criterion)\n",
    "print(\"Optimizer:\", optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc623a2b-20d4-4de2-a9b6-ecfb65f67832",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_correct = 0\n",
    "    running_total = 0\n",
    "\n",
    "    for images, labels in loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, preds = outputs.max(1)\n",
    "        running_correct += (preds == labels).sum().item()\n",
    "        running_total += labels.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / running_total\n",
    "    epoch_acc = running_correct / running_total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_one_epoch(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_correct = 0\n",
    "    running_total = 0\n",
    "\n",
    "    for images, labels in loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, preds = outputs.max(1)\n",
    "        running_correct += (preds == labels).sum().item()\n",
    "        running_total += labels.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / running_total\n",
    "    epoch_acc = running_correct / running_total\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c30d04db-644a-4c30-9221-b1113ff3d709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Epoch 1/5 =====\n",
      "  [ 10.0%] Batch 181/1817 - loss: 2.7166, acc: 0.3258\n",
      "  [ 19.9%] Batch 362/1817 - loss: 1.7384, acc: 0.4659\n",
      "  [ 29.9%] Batch 543/1817 - loss: 0.7490, acc: 0.5436\n",
      "  [ 39.8%] Batch 724/1817 - loss: 2.0234, acc: 0.5953\n",
      "  [ 49.8%] Batch 905/1817 - loss: 1.1550, acc: 0.6372\n",
      "  [ 59.8%] Batch 1086/1817 - loss: 0.6894, acc: 0.6687\n",
      "  [ 69.7%] Batch 1267/1817 - loss: 0.6966, acc: 0.6938\n",
      "  [ 79.7%] Batch 1448/1817 - loss: 0.4722, acc: 0.7151\n",
      "  [ 89.7%] Batch 1629/1817 - loss: 0.3734, acc: 0.7338\n",
      "  [ 99.6%] Batch 1810/1817 - loss: 0.6523, acc: 0.7504\n",
      "  [100.0%] Batch 1817/1817 - loss: 0.7508, acc: 0.7509\n",
      "\n",
      "[Epoch 1] train_loss=1.4308  train_acc=0.7509\n",
      "[Epoch 1] val_loss=0.3911  val_acc=0.9218\n",
      "  New BEST model (val_acc=0.9218)\n",
      "\n",
      "===== Epoch 2/5 =====\n",
      "  [ 10.0%] Batch 181/1817 - loss: 0.7683, acc: 0.9296\n",
      "  [ 19.9%] Batch 362/1817 - loss: 0.2478, acc: 0.9303\n",
      "  [ 29.9%] Batch 543/1817 - loss: 0.2410, acc: 0.9334\n",
      "  [ 39.8%] Batch 724/1817 - loss: 0.1974, acc: 0.9342\n",
      "  [ 49.8%] Batch 905/1817 - loss: 0.3147, acc: 0.9353\n",
      "  [ 59.8%] Batch 1086/1817 - loss: 0.3216, acc: 0.9369\n",
      "  [ 69.7%] Batch 1267/1817 - loss: 0.1870, acc: 0.9385\n",
      "  [ 79.7%] Batch 1448/1817 - loss: 0.4390, acc: 0.9402\n",
      "  [ 89.7%] Batch 1629/1817 - loss: 0.3784, acc: 0.9418\n",
      "  [ 99.6%] Batch 1810/1817 - loss: 0.0246, acc: 0.9429\n",
      "  [100.0%] Batch 1817/1817 - loss: 0.6871, acc: 0.9428\n",
      "\n",
      "[Epoch 2] train_loss=0.3138  train_acc=0.9428\n",
      "[Epoch 2] val_loss=0.1938  val_acc=0.9619\n",
      "  New BEST model (val_acc=0.9619)\n",
      "\n",
      "===== Epoch 3/5 =====\n",
      "  [ 10.0%] Batch 181/1817 - loss: 0.1517, acc: 0.9706\n",
      "  [ 19.9%] Batch 362/1817 - loss: 0.0208, acc: 0.9716\n",
      "  [ 29.9%] Batch 543/1817 - loss: 0.1425, acc: 0.9716\n",
      "  [ 39.8%] Batch 724/1817 - loss: 0.2901, acc: 0.9703\n",
      "  [ 49.8%] Batch 905/1817 - loss: 0.2136, acc: 0.9705\n",
      "  [ 59.8%] Batch 1086/1817 - loss: 0.1503, acc: 0.9709\n",
      "  [ 69.7%] Batch 1267/1817 - loss: 0.2308, acc: 0.9705\n",
      "  [ 79.7%] Batch 1448/1817 - loss: 0.0342, acc: 0.9708\n",
      "  [ 89.7%] Batch 1629/1817 - loss: 0.0467, acc: 0.9707\n",
      "  [ 99.6%] Batch 1810/1817 - loss: 0.0667, acc: 0.9707\n",
      "  [100.0%] Batch 1817/1817 - loss: 0.1144, acc: 0.9707\n",
      "\n",
      "[Epoch 3] train_loss=0.1535  train_acc=0.9707\n",
      "[Epoch 3] val_loss=0.1391  val_acc=0.9705\n",
      "  New BEST model (val_acc=0.9705)\n",
      "\n",
      "===== Epoch 4/5 =====\n",
      "  [ 10.0%] Batch 181/1817 - loss: 0.0524, acc: 0.9836\n",
      "  [ 19.9%] Batch 362/1817 - loss: 0.0317, acc: 0.9839\n",
      "  [ 29.9%] Batch 543/1817 - loss: 0.0688, acc: 0.9831\n",
      "  [ 39.8%] Batch 724/1817 - loss: 0.1629, acc: 0.9825\n",
      "  [ 49.8%] Batch 905/1817 - loss: 0.0187, acc: 0.9837\n",
      "  [ 59.8%] Batch 1086/1817 - loss: 0.0260, acc: 0.9834\n",
      "  [ 69.7%] Batch 1267/1817 - loss: 0.0775, acc: 0.9826\n",
      "  [ 79.7%] Batch 1448/1817 - loss: 0.0880, acc: 0.9823\n",
      "  [ 89.7%] Batch 1629/1817 - loss: 0.0392, acc: 0.9821\n",
      "  [ 99.6%] Batch 1810/1817 - loss: 0.1145, acc: 0.9817\n",
      "  [100.0%] Batch 1817/1817 - loss: 0.1005, acc: 0.9818\n",
      "\n",
      "[Epoch 4] train_loss=0.0926  train_acc=0.9818\n",
      "[Epoch 4] val_loss=0.1237  val_acc=0.9732\n",
      "  New BEST model (val_acc=0.9732)\n",
      "\n",
      "===== Epoch 5/5 =====\n",
      "  [ 10.0%] Batch 181/1817 - loss: 0.0769, acc: 0.9891\n",
      "  [ 19.9%] Batch 362/1817 - loss: 0.0253, acc: 0.9915\n",
      "  [ 29.9%] Batch 543/1817 - loss: 0.0921, acc: 0.9907\n",
      "  [ 39.8%] Batch 724/1817 - loss: 0.0782, acc: 0.9902\n",
      "  [ 49.8%] Batch 905/1817 - loss: 0.0051, acc: 0.9897\n",
      "  [ 59.8%] Batch 1086/1817 - loss: 0.0835, acc: 0.9890\n",
      "  [ 69.7%] Batch 1267/1817 - loss: 0.0629, acc: 0.9889\n",
      "  [ 79.7%] Batch 1448/1817 - loss: 0.0047, acc: 0.9887\n",
      "  [ 89.7%] Batch 1629/1817 - loss: 0.0279, acc: 0.9882\n",
      "  [ 99.6%] Batch 1810/1817 - loss: 0.0624, acc: 0.9881\n",
      "  [100.0%] Batch 1817/1817 - loss: 0.1230, acc: 0.9881\n",
      "\n",
      "[Epoch 5] train_loss=0.0610  train_acc=0.9881\n",
      "[Epoch 5] val_loss=0.1155  val_acc=0.9743\n",
      "  New BEST model (val_acc=0.9743)\n",
      "\n",
      "=== Training Complete ===\n",
      "Best val_acc = 0.9743\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "\n",
    "best_val_acc = 0.0\n",
    "best_state_dict = None\n",
    "\n",
    "history = {\n",
    "    \"train_loss\": [],\n",
    "    \"train_acc\": [],\n",
    "    \"val_loss\": [],\n",
    "    \"val_acc\": []\n",
    "}\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\n===== Epoch {epoch+1}/{EPOCHS} =====\")\n",
    "    \n",
    "    # TRAINING\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_correct = 0\n",
    "    running_total = 0\n",
    "    \n",
    "    total_batches = len(train_loader)\n",
    "    print_every = max(1, total_batches // 10)  # Print 10 times per epoch\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, preds = outputs.max(1)\n",
    "        running_correct += (preds == labels).sum().item()\n",
    "        running_total += labels.size(0)\n",
    "        \n",
    "        # Print progress\n",
    "        if (batch_idx + 1) % print_every == 0 or (batch_idx + 1) == total_batches:\n",
    "            batch_acc = running_correct / running_total\n",
    "            progress = (batch_idx + 1) / total_batches * 100\n",
    "            print(f\"  [{progress:5.1f}%] Batch {batch_idx+1}/{total_batches} - loss: {loss.item():.4f}, acc: {batch_acc:.4f}\")\n",
    "\n",
    "    train_loss = running_loss / running_total\n",
    "    train_acc = running_correct / running_total\n",
    "    \n",
    "    # VALIDATION\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_correct = 0\n",
    "    running_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, preds = outputs.max(1)\n",
    "            running_correct += (preds == labels).sum().item()\n",
    "            running_total += labels.size(0)\n",
    "    \n",
    "    val_loss = running_loss / running_total\n",
    "    val_acc = running_correct / running_total\n",
    "    \n",
    "    history[\"train_loss\"].append(train_loss)\n",
    "    history[\"train_acc\"].append(train_acc)\n",
    "    history[\"val_loss\"].append(val_loss)\n",
    "    history[\"val_acc\"].append(val_acc)\n",
    "    \n",
    "    print(f\"\\n[Epoch {epoch+1}] train_loss={train_loss:.4f}  train_acc={train_acc:.4f}\")\n",
    "    print(f\"[Epoch {epoch+1}] val_loss={val_loss:.4f}  val_acc={val_acc:.4f}\")\n",
    "    \n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_state_dict = {k: v.cpu() for k, v in model.state_dict().items()}\n",
    "        print(f\"  New BEST model (val_acc={best_val_acc:.4f})\")\n",
    "\n",
    "print(\"\\n=== Training Complete ===\")\n",
    "print(f\"Best val_acc = {best_val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c81665ae-3faf-4dad-bba0-3359312fd4f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved best model -> best_custom_cnn_image_resnet18.pth\n",
      "Saved class_to_idx_image_resnet18.json\n",
      "\n",
      "Training curves:\n",
      "Epoch 01: train_acc=0.7509, val_acc=0.9218, train_loss=1.4308, val_loss=0.3911\n",
      "Epoch 02: train_acc=0.9428, val_acc=0.9619, train_loss=0.3138, val_loss=0.1938\n",
      "Epoch 03: train_acc=0.9707, val_acc=0.9705, train_loss=0.1535, val_loss=0.1391\n",
      "Epoch 04: train_acc=0.9818, val_acc=0.9732, train_loss=0.0926, val_loss=0.1237\n",
      "Epoch 05: train_acc=0.9881, val_acc=0.9743, train_loss=0.0610, val_loss=0.1155\n"
     ]
    }
   ],
   "source": [
    "if best_state_dict is not None:\n",
    "    torch.save(best_state_dict, \"best_custom_cnn_image_resnet18.pth\")\n",
    "    print(\"Saved best model -> best_custom_cnn_image_resnet18.pth\")\n",
    "\n",
    "with open(\"class_to_idx_image_resnet18.json\", \"w\") as f:\n",
    "    json.dump(class_to_idx, f, indent=2)\n",
    "print(\"Saved class_to_idx_image_resnet18.json\")\n",
    "\n",
    "print(\"\\nTraining curves:\")\n",
    "for i in range(EPOCHS):\n",
    "    print(\n",
    "        f\"Epoch {i+1:02d}: \"\n",
    "        f\"train_acc={history['train_acc'][i]:.4f}, \"\n",
    "        f\"val_acc={history['val_acc'][i]:.4f}, \"\n",
    "        f\"train_loss={history['train_loss'][i]:.4f}, \"\n",
    "        f\"val_loss={history['val_loss'][i]:.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc1c535-c8f1-45ba-a833-3a97daacb98e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (GPU)",
   "language": "python",
   "name": "my_gpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
