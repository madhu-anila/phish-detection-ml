{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c40db48-bc8d-4e9c-975e-562b6ea1d732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in c:\\users\\srihari\\anaconda3\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\srihari\\anaconda3\\lib\\site-packages (from tqdm) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a7ab48e-5b1a-4593-87d4-ef4bc3251d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "CUDA Available: True\n",
      "Device Count: 1\n",
      "Current Device: 0\n",
      "Device Name: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "Device Capability: (8, 9)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from glob import glob\n",
    "import xml.etree.ElementTree as ET\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'  # Force GPU 0 (NVIDIA)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"Device Count:\", torch.cuda.device_count())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Current Device:\", torch.cuda.current_device())\n",
    "    print(\"Device Name:\", torch.cuda.get_device_name(0))\n",
    "    print(\"Device Capability:\", torch.cuda.get_device_capability(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94856d52-c955-485d-b915-2665887175bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE_DIR:      C:\\Users\\Srihari\\ENPM703FinalProject\n",
      "OPENLOGO_DIR:  C:\\Users\\Srihari\\ENPM703FinalProject\\openlogo\n",
      "JPEG_DIR:      C:\\Users\\Srihari\\ENPM703FinalProject\\openlogo\\JPEGImages\n",
      "ANNO_DIR:      C:\\Users\\Srihari\\ENPM703FinalProject\\openlogo\\Annotations\n",
      "IMAGESET_DIR:  C:\\Users\\Srihari\\ENPM703FinalProject\\openlogo\\ImageSets\\class_sep\n",
      "\n",
      "Sample images: ['C:\\\\Users\\\\Srihari\\\\ENPM703FinalProject\\\\openlogo\\\\JPEGImages\\\\1008198576.jpg', 'C:\\\\Users\\\\Srihari\\\\ENPM703FinalProject\\\\openlogo\\\\JPEGImages\\\\1016381746.jpg']\n",
      "Sample annos:  ['C:\\\\Users\\\\Srihari\\\\ENPM703FinalProject\\\\openlogo\\\\Annotations\\\\1008198576.xml', 'C:\\\\Users\\\\Srihari\\\\ENPM703FinalProject\\\\openlogo\\\\Annotations\\\\1016381746.xml']\n"
     ]
    }
   ],
   "source": [
    "# Path configuration\n",
    "BASE_DIR = os.path.abspath(\".\")\n",
    "OPENLOGO_DIR = os.path.join(BASE_DIR, \"openlogo\")\n",
    "\n",
    "JPEG_DIR = os.path.join(OPENLOGO_DIR, \"JPEGImages\")\n",
    "ANNO_DIR = os.path.join(OPENLOGO_DIR, \"Annotations\")\n",
    "IMAGESET_DIR = os.path.join(OPENLOGO_DIR, \"ImageSets\", \"class_sep\")\n",
    "\n",
    "print(\"BASE_DIR:     \", BASE_DIR)\n",
    "print(\"OPENLOGO_DIR: \", OPENLOGO_DIR)\n",
    "print(\"JPEG_DIR:     \", JPEG_DIR)\n",
    "print(\"ANNO_DIR:     \", ANNO_DIR)\n",
    "print(\"IMAGESET_DIR: \", IMAGESET_DIR)\n",
    "\n",
    "# Sanity check\n",
    "some_imgs = glob(os.path.join(JPEG_DIR, \"*.jpg\"))[:5]\n",
    "some_annos = glob(os.path.join(ANNO_DIR, \"*.xml\"))[:5]\n",
    "print(\"\\nSample images:\", some_imgs[:2])\n",
    "print(\"Sample annos: \", some_annos[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c5dd7a8-16da-40e6-9a36-d8d01692328b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing parse_voc_xml on: C:\\Users\\Srihari\\ENPM703FinalProject\\openlogo\\Annotations\\1008198576.xml\n",
      "[{'class_name': 'guinness', 'bbox': [333, 238, 595, 414]}]\n"
     ]
    }
   ],
   "source": [
    "def parse_voc_xml(xml_path):\n",
    "    \"\"\"\n",
    "    Parse one XML annotation. Return a list of objects.\n",
    "    Each object is: {'class_name': str, 'bbox': [xmin, ymin, xmax, ymax]}\n",
    "    \"\"\"\n",
    "    tree = ET.parse(xml_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    objects = []\n",
    "    for obj in root.findall(\"object\"):\n",
    "        cls_name = obj.find(\"name\").text.strip()\n",
    "\n",
    "        bbox_node = obj.find(\"bndbox\")\n",
    "        xmin = int(float(bbox_node.find(\"xmin\").text))\n",
    "        ymin = int(float(bbox_node.find(\"ymin\").text))\n",
    "        xmax = int(float(bbox_node.find(\"xmax\").text))\n",
    "        ymax = int(float(bbox_node.find(\"ymax\").text))\n",
    "\n",
    "        objects.append({\n",
    "            \"class_name\": cls_name,\n",
    "            \"bbox\": [xmin, ymin, xmax, ymax]\n",
    "        })\n",
    "\n",
    "    return objects\n",
    "\n",
    "# Test\n",
    "if len(some_annos) > 0:\n",
    "    test_xml = some_annos[0]\n",
    "    print(\"Testing parse_voc_xml on:\", test_xml)\n",
    "    print(parse_voc_xml(test_xml)[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d47309c0-caa2-4240-b404-fc7aa9e86430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found train list files: 353\n",
      "Discovered classes: 352\n",
      "Total samples: 72652 -> train 58121, val 14531\n",
      "num_classes: 352\n",
      "\n",
      "Sample: {'img_path': 'C:\\\\Users\\\\Srihari\\\\ENPM703FinalProject\\\\openlogo\\\\JPEGImages\\\\CPA_Australia_sportslogo_22.jpg', 'bbox': [1, 330, 54, 352], 'class_name': 'cpa_australia', 'label': 104}\n"
     ]
    }
   ],
   "source": [
    "def build_logo_samples(imageset_dir, jpeg_dir, anno_dir, \n",
    "                       train_suffix=\"_train.txt\", test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Build train/val samples from OpenLogo dataset.\n",
    "    Returns: train_samples, val_samples, class_to_idx\n",
    "    \"\"\"\n",
    "    all_samples = []\n",
    "\n",
    "    list_files = glob(os.path.join(imageset_dir, f\"*{train_suffix}\"))\n",
    "    list_files.sort()\n",
    "    print(\"Found train list files:\", len(list_files))\n",
    "\n",
    "    for lf in list_files:\n",
    "        with open(lf, \"r\") as f:\n",
    "            ids = [line.strip() for line in f if len(line.strip()) > 0]\n",
    "\n",
    "        for img_id in ids:\n",
    "            img_path = os.path.join(jpeg_dir, img_id + \".jpg\")\n",
    "            xml_path = os.path.join(anno_dir, img_id + \".xml\")\n",
    "\n",
    "            if not (os.path.exists(img_path) and os.path.exists(xml_path)):\n",
    "                continue\n",
    "\n",
    "            objs = parse_voc_xml(xml_path)\n",
    "\n",
    "            for obj in objs:\n",
    "                cls_name = obj[\"class_name\"]\n",
    "                bbox = obj[\"bbox\"]\n",
    "\n",
    "                all_samples.append({\n",
    "                    \"img_path\": img_path,\n",
    "                    \"bbox\": bbox,\n",
    "                    \"class_name\": cls_name\n",
    "                })\n",
    "\n",
    "    # Build class mapping\n",
    "    classes = sorted(list({s[\"class_name\"] for s in all_samples}))\n",
    "    class_to_idx = {c: i for i, c in enumerate(classes)}\n",
    "    print(\"Discovered classes:\", len(class_to_idx))\n",
    "\n",
    "    # Add numeric labels\n",
    "    for s in all_samples:\n",
    "        s[\"label\"] = class_to_idx[s[\"class_name\"]]\n",
    "\n",
    "    # Stratified split\n",
    "    y = [s[\"label\"] for s in all_samples]\n",
    "    train_s, val_s = train_test_split(\n",
    "        all_samples,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "        stratify=y\n",
    "    )\n",
    "\n",
    "    print(f\"Total samples: {len(all_samples)} -> train {len(train_s)}, val {len(val_s)}\")\n",
    "    return train_s, val_s, class_to_idx\n",
    "\n",
    "\n",
    "train_samples, val_samples, class_to_idx = build_logo_samples(\n",
    "    IMAGESET_DIR, JPEG_DIR, ANNO_DIR,\n",
    "    train_suffix=\"_train.txt\", test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "num_classes = len(class_to_idx)\n",
    "print(\"num_classes:\", num_classes)\n",
    "print(\"\\nSample:\", train_samples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d56fb7d-e64f-46c1-83e7-00b53ddf1119",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogoCropDataset(Dataset):\n",
    "    def __init__(self, samples, transform=None):\n",
    "        self.samples = samples\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        s = self.samples[idx]\n",
    "        path = s[\"img_path\"]\n",
    "        xmin, ymin, xmax, ymax = s[\"bbox\"]\n",
    "        label = s[\"label\"]\n",
    "\n",
    "        # Load image\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "\n",
    "        # Defensive clamp\n",
    "        w, h = img.size\n",
    "        xmin_c = max(0, min(xmin, w - 1))\n",
    "        ymin_c = max(0, min(ymin, h - 1))\n",
    "        xmax_c = max(0, min(xmax, w))\n",
    "        ymax_c = max(0, min(ymax, h))\n",
    "\n",
    "        if xmax_c <= xmin_c or ymax_c <= ymin_c:\n",
    "            crop = img\n",
    "        else:\n",
    "            crop = img.crop((xmin_c, ymin_c, xmax_c, ymax_c))\n",
    "\n",
    "        if self.transform is not None:\n",
    "            crop = self.transform(crop)\n",
    "\n",
    "        return crop, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "542d442a-0a00-4179-8bbf-c8e888e66054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 1817 Val batches: 455\n"
     ]
    }
   ],
   "source": [
    "train_transform = T.Compose([\n",
    "    T.Resize((224, 224)),\n",
    "    T.RandomHorizontalFlip(p=0.5),\n",
    "    T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.05),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "val_transform = T.Compose([\n",
    "    T.Resize((224, 224)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "train_ds = LogoCropDataset(train_samples, transform=train_transform)\n",
    "val_ds = LogoCropDataset(val_samples, transform=val_transform)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=(device.type == \"cuda\")\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=(device.type == \"cuda\")\n",
    ")\n",
    "\n",
    "print(\"Train batches:\", len(train_loader), \"Val batches:\", len(val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32655eb8-63e4-4b2e-9dd3-b33bf1a74593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Initializing Custom CNN for Logo Classification\n",
      "============================================================\n",
      "\n",
      "Total parameters: 8,083,872\n",
      "Trainable parameters: 8,083,872\n",
      "Model size: ~30.84 MB\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class CustomCNN_Logo(nn.Module):\n",
    "    \"\"\"\n",
    "    Custom CNN that WORKS - simplified but effective architecture\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=352, dropout=0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Conv Block 1\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2)  # 224 -> 112\n",
    "        )\n",
    "        \n",
    "        # Conv Block 2\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2)  # 112 -> 56\n",
    "        )\n",
    "        \n",
    "        # Conv Block 3\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2)  # 56 -> 28\n",
    "        )\n",
    "        \n",
    "        # Conv Block 4\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2)  # 28 -> 14\n",
    "        )\n",
    "        \n",
    "        # Global Average Pooling\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        # Classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Initializing Custom CNN for Logo Classification\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "model = CustomCNN_Logo(num_classes=num_classes, dropout=0.3).to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Model size: ~{total_params * 4 / 1024 / 1024:.2f} MB\")\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5833bf02-1f68-4701-bdfe-d86014b4fdae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer: Adam\n",
      "Initial Learning Rate: 3e-4\n",
      "Weight Decay: 1e-4\n",
      "LR Scheduler: StepLR (decay by 0.5 every 5 epochs)\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Higher initial learning rate for faster convergence\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-4, weight_decay=1e-4)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "print(\"Optimizer: Adam\")\n",
    "print(\"Initial Learning Rate: 3e-4\")\n",
    "print(\"Weight Decay: 1e-4\")\n",
    "print(\"LR Scheduler: StepLR (decay by 0.5 every 5 epochs)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc623a2b-20d4-4de2-a9b6-ecfb65f67832",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_correct = 0\n",
    "    running_total = 0\n",
    "\n",
    "    for images, labels in loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, preds = outputs.max(1)\n",
    "        running_correct += (preds == labels).sum().item()\n",
    "        running_total += labels.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / running_total\n",
    "    epoch_acc = running_correct / running_total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_one_epoch(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_correct = 0\n",
    "    running_total = 0\n",
    "\n",
    "    for images, labels in loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, preds = outputs.max(1)\n",
    "        running_correct += (preds == labels).sum().item()\n",
    "        running_total += labels.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / running_total\n",
    "    epoch_acc = running_correct / running_total\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c30d04db-644a-4c30-9221-b1113ff3d709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training for 10 epochs...\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Epoch 1/10\n",
      "============================================================\n",
      "  [ 200/1817] loss: 4.8026, acc: 0.0533\n",
      "  [ 400/1817] loss: 4.1379, acc: 0.0631\n",
      "  [ 600/1817] loss: 4.5574, acc: 0.0723\n",
      "  [ 800/1817] loss: 5.0171, acc: 0.0811\n",
      "  [1000/1817] loss: 4.4030, acc: 0.0882\n",
      "  [1200/1817] loss: 3.9808, acc: 0.0955\n",
      "  [1400/1817] loss: 3.6371, acc: 0.1037\n",
      "  [1600/1817] loss: 4.1331, acc: 0.1131\n",
      "  [1800/1817] loss: 4.2470, acc: 0.1216\n",
      "\n",
      "============================================================\n",
      "[Epoch 1/10 Summary]\n",
      "  Train Loss: 4.4316 | Train Acc: 0.1224\n",
      "  Val Loss:   3.8099 | Val Acc:   0.2081\n",
      "  Time: 1338.6s\n",
      "  Current LR: 0.000300\n",
      "  New BEST model! (val_acc=0.2081)\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Epoch 2/10\n",
      "============================================================\n",
      "  [ 200/1817] loss: 4.4671, acc: 0.2162\n",
      "  [ 400/1817] loss: 3.7377, acc: 0.2200\n",
      "  [ 600/1817] loss: 2.9227, acc: 0.2311\n",
      "  [ 800/1817] loss: 3.2133, acc: 0.2399\n",
      "  [1000/1817] loss: 3.2529, acc: 0.2503\n",
      "  [1200/1817] loss: 3.2759, acc: 0.2624\n",
      "  [1400/1817] loss: 3.0216, acc: 0.2690\n",
      "  [1600/1817] loss: 2.8207, acc: 0.2759\n",
      "  [1800/1817] loss: 3.4289, acc: 0.2819\n",
      "\n",
      "============================================================\n",
      "[Epoch 2/10 Summary]\n",
      "  Train Loss: 3.3780 | Train Acc: 0.2823\n",
      "  Val Loss:   2.7908 | Val Acc:   0.3824\n",
      "  Time: 1308.4s\n",
      "  Current LR: 0.000300\n",
      "  New BEST model! (val_acc=0.3824)\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Epoch 3/10\n",
      "============================================================\n",
      "  [ 200/1817] loss: 3.1542, acc: 0.3498\n",
      "  [ 400/1817] loss: 2.6600, acc: 0.3600\n",
      "  [ 600/1817] loss: 2.8552, acc: 0.3640\n",
      "  [ 800/1817] loss: 3.0336, acc: 0.3672\n",
      "  [1000/1817] loss: 2.0208, acc: 0.3721\n",
      "  [1200/1817] loss: 2.1948, acc: 0.3779\n",
      "  [1400/1817] loss: 2.6581, acc: 0.3830\n",
      "  [1600/1817] loss: 2.7289, acc: 0.3900\n",
      "  [1800/1817] loss: 3.1793, acc: 0.3955\n",
      "\n",
      "============================================================\n",
      "[Epoch 3/10 Summary]\n",
      "  Train Loss: 2.7155 | Train Acc: 0.3958\n",
      "  Val Loss:   2.3092 | Val Acc:   0.4727\n",
      "  Time: 1310.2s\n",
      "  Current LR: 0.000300\n",
      "  New BEST model! (val_acc=0.4727)\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Epoch 4/10\n",
      "============================================================\n",
      "  [ 200/1817] loss: 2.7072, acc: 0.4516\n",
      "  [ 400/1817] loss: 2.9441, acc: 0.4519\n",
      "  [ 600/1817] loss: 2.5647, acc: 0.4562\n",
      "  [ 800/1817] loss: 1.9167, acc: 0.4626\n",
      "  [1000/1817] loss: 1.9528, acc: 0.4655\n",
      "  [1200/1817] loss: 1.7213, acc: 0.4694\n",
      "  [1400/1817] loss: 2.1667, acc: 0.4741\n",
      "  [1600/1817] loss: 2.0117, acc: 0.4773\n",
      "  [1800/1817] loss: 3.1542, acc: 0.4810\n",
      "\n",
      "============================================================\n",
      "[Epoch 4/10 Summary]\n",
      "  Train Loss: 2.2738 | Train Acc: 0.4814\n",
      "  Val Loss:   1.9102 | Val Acc:   0.5641\n",
      "  Time: 1313.2s\n",
      "  Current LR: 0.000300\n",
      "  New BEST model! (val_acc=0.5641)\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Epoch 5/10\n",
      "============================================================\n",
      "  [ 200/1817] loss: 2.1838, acc: 0.5286\n",
      "  [ 400/1817] loss: 2.4361, acc: 0.5238\n",
      "  [ 600/1817] loss: 2.3858, acc: 0.5292\n",
      "  [ 800/1817] loss: 2.1674, acc: 0.5319\n",
      "  [1000/1817] loss: 1.4891, acc: 0.5363\n",
      "  [1200/1817] loss: 1.5419, acc: 0.5391\n",
      "  [1400/1817] loss: 2.2341, acc: 0.5415\n",
      "  [1600/1817] loss: 2.7599, acc: 0.5446\n",
      "  [1800/1817] loss: 1.7689, acc: 0.5485\n",
      "\n",
      "============================================================\n",
      "[Epoch 5/10 Summary]\n",
      "  Train Loss: 1.9397 | Train Acc: 0.5487\n",
      "  Val Loss:   1.6334 | Val Acc:   0.6121\n",
      "  Time: 1313.3s\n",
      "  Current LR: 0.000150\n",
      "  New BEST model! (val_acc=0.6121)\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Epoch 6/10\n",
      "============================================================\n",
      "  [ 200/1817] loss: 1.6094, acc: 0.6128\n",
      "  [ 400/1817] loss: 1.8172, acc: 0.6132\n",
      "  [ 600/1817] loss: 0.9281, acc: 0.6151\n",
      "  [ 800/1817] loss: 1.3699, acc: 0.6166\n",
      "  [1000/1817] loss: 2.1044, acc: 0.6190\n",
      "  [1200/1817] loss: 1.3009, acc: 0.6203\n",
      "  [1400/1817] loss: 1.4571, acc: 0.6224\n",
      "  [1600/1817] loss: 1.0780, acc: 0.6242\n",
      "  [1800/1817] loss: 1.9822, acc: 0.6253\n",
      "\n",
      "============================================================\n",
      "[Epoch 6/10 Summary]\n",
      "  Train Loss: 1.5827 | Train Acc: 0.6255\n",
      "  Val Loss:   1.3089 | Val Acc:   0.6922\n",
      "  Time: 1313.4s\n",
      "  Current LR: 0.000150\n",
      "  New BEST model! (val_acc=0.6922)\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Epoch 7/10\n",
      "============================================================\n",
      "  [ 200/1817] loss: 1.5693, acc: 0.6542\n",
      "  [ 400/1817] loss: 1.0380, acc: 0.6527\n",
      "  [ 600/1817] loss: 1.6693, acc: 0.6535\n",
      "  [ 800/1817] loss: 1.6735, acc: 0.6529\n",
      "  [1000/1817] loss: 1.4307, acc: 0.6550\n",
      "  [1200/1817] loss: 1.3385, acc: 0.6570\n",
      "  [1400/1817] loss: 0.5432, acc: 0.6591\n",
      "  [1600/1817] loss: 1.4609, acc: 0.6594\n",
      "  [1800/1817] loss: 0.9174, acc: 0.6610\n",
      "\n",
      "============================================================\n",
      "[Epoch 7/10 Summary]\n",
      "  Train Loss: 1.4212 | Train Acc: 0.6611\n",
      "  Val Loss:   1.1691 | Val Acc:   0.7251\n",
      "  Time: 1316.1s\n",
      "  Current LR: 0.000150\n",
      "  New BEST model! (val_acc=0.7251)\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Epoch 8/10\n",
      "============================================================\n",
      "  [ 200/1817] loss: 0.8886, acc: 0.6741\n",
      "  [ 400/1817] loss: 2.2309, acc: 0.6823\n",
      "  [ 600/1817] loss: 1.3482, acc: 0.6795\n",
      "  [ 800/1817] loss: 1.0198, acc: 0.6819\n",
      "  [1000/1817] loss: 1.1759, acc: 0.6823\n",
      "  [1200/1817] loss: 1.2874, acc: 0.6846\n",
      "  [1400/1817] loss: 0.8965, acc: 0.6867\n",
      "  [1600/1817] loss: 1.1540, acc: 0.6894\n",
      "  [1800/1817] loss: 1.8825, acc: 0.6901\n",
      "\n",
      "============================================================\n",
      "[Epoch 8/10 Summary]\n",
      "  Train Loss: 1.2916 | Train Acc: 0.6901\n",
      "  Val Loss:   1.0680 | Val Acc:   0.7471\n",
      "  Time: 1349.2s\n",
      "  Current LR: 0.000150\n",
      "  New BEST model! (val_acc=0.7471)\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Epoch 9/10\n",
      "============================================================\n",
      "  [ 200/1817] loss: 0.9409, acc: 0.7042\n",
      "  [ 400/1817] loss: 1.8232, acc: 0.7049\n",
      "  [ 600/1817] loss: 1.6184, acc: 0.7053\n",
      "  [ 800/1817] loss: 1.6532, acc: 0.7085\n",
      "  [1000/1817] loss: 0.9745, acc: 0.7096\n",
      "  [1200/1817] loss: 0.9730, acc: 0.7092\n",
      "  [1400/1817] loss: 1.4081, acc: 0.7100\n",
      "  [1600/1817] loss: 1.2133, acc: 0.7102\n",
      "  [1800/1817] loss: 0.5431, acc: 0.7120\n",
      "\n",
      "============================================================\n",
      "[Epoch 9/10 Summary]\n",
      "  Train Loss: 1.1840 | Train Acc: 0.7117\n",
      "  Val Loss:   1.0529 | Val Acc:   0.7503\n",
      "  Time: 1350.4s\n",
      "  Current LR: 0.000150\n",
      "  New BEST model! (val_acc=0.7503)\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Epoch 10/10\n",
      "============================================================\n",
      "  [ 200/1817] loss: 1.0269, acc: 0.7362\n",
      "  [ 400/1817] loss: 0.6500, acc: 0.7347\n",
      "  [ 600/1817] loss: 0.9339, acc: 0.7351\n",
      "  [ 800/1817] loss: 1.1913, acc: 0.7317\n",
      "  [1000/1817] loss: 1.0139, acc: 0.7318\n",
      "  [1200/1817] loss: 0.6374, acc: 0.7339\n",
      "  [1400/1817] loss: 0.7226, acc: 0.7345\n",
      "  [1600/1817] loss: 1.1295, acc: 0.7344\n",
      "  [1800/1817] loss: 0.8886, acc: 0.7356\n",
      "\n",
      "============================================================\n",
      "[Epoch 10/10 Summary]\n",
      "  Train Loss: 1.0859 | Train Acc: 0.7356\n",
      "  Val Loss:   0.9793 | Val Acc:   0.7630\n",
      "  Time: 1358.0s\n",
      "  Current LR: 0.000075\n",
      "  New BEST model! (val_acc=0.7630)\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "=== Training Complete ===\n",
      "============================================================\n",
      "Best Validation Accuracy: 0.7630 (76.30%)\n",
      "Total Training Time: 221.2 minutes\n",
      "Average Time per Epoch: 1327.1 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "EPOCHS = 10  # More epochs for better accuracy\n",
    "\n",
    "best_val_acc = 0.0\n",
    "best_state_dict = None\n",
    "\n",
    "history = {\n",
    "    \"train_loss\": [],\n",
    "    \"train_acc\": [],\n",
    "    \"val_loss\": [],\n",
    "    \"val_acc\": []\n",
    "}\n",
    "\n",
    "print(f\"\\nStarting training for {EPOCHS} epochs...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "total_start_time = time.time()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_start_time = time.time()\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # TRAINING\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_correct = 0\n",
    "    running_total = 0\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, preds = outputs.max(1)\n",
    "        running_correct += (preds == labels).sum().item()\n",
    "        running_total += labels.size(0)\n",
    "        \n",
    "        # Print progress every 200 batches\n",
    "        if (batch_idx + 1) % 200 == 0:\n",
    "            batch_acc = running_correct / running_total\n",
    "            print(f\"  [{batch_idx+1:4d}/{len(train_loader)}] \"\n",
    "                  f\"loss: {loss.item():.4f}, acc: {batch_acc:.4f}\")\n",
    "\n",
    "    train_loss = running_loss / running_total\n",
    "    train_acc = running_correct / running_total\n",
    "    \n",
    "    # VALIDATION\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_correct = 0\n",
    "    running_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, preds = outputs.max(1)\n",
    "            running_correct += (preds == labels).sum().item()\n",
    "            running_total += labels.size(0)\n",
    "    \n",
    "    val_loss = running_loss / running_total\n",
    "    val_acc = running_correct / running_total\n",
    "    \n",
    "    # Update learning rate based on validation accuracy\n",
    "    scheduler.step()\n",
    "    \n",
    "    history[\"train_loss\"].append(train_loss)\n",
    "    history[\"train_acc\"].append(train_acc)\n",
    "    history[\"val_loss\"].append(val_loss)\n",
    "    history[\"val_acc\"].append(val_acc)\n",
    "    \n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"[Epoch {epoch+1}/{EPOCHS} Summary]\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"  Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.4f}\")\n",
    "    print(f\"  Time: {epoch_time:.1f}s\")\n",
    "    print(f\"  Current LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "    \n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_state_dict = {k: v.cpu() for k, v in model.state_dict().items()}\n",
    "        print(f\"  New BEST model! (val_acc={best_val_acc:.4f})\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "total_time = time.time() - total_start_time\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"=== Training Complete ===\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Best Validation Accuracy: {best_val_acc:.4f} ({best_val_acc*100:.2f}%)\")\n",
    "print(f\"Total Training Time: {total_time/60:.1f} minutes\")\n",
    "print(f\"Average Time per Epoch: {total_time/EPOCHS:.1f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c81665ae-3faf-4dad-bba0-3359312fd4f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved best model -> best_custom_cnn_image_custom.pth\n",
      "Saved class_to_idx_image_custom.json\n",
      "\n",
      "Training curves:\n",
      "Epoch 01: train_acc=0.1224, val_acc=0.2081, train_loss=4.4316, val_loss=3.8099\n",
      "Epoch 02: train_acc=0.2823, val_acc=0.3824, train_loss=3.3780, val_loss=2.7908\n",
      "Epoch 03: train_acc=0.3958, val_acc=0.4727, train_loss=2.7155, val_loss=2.3092\n",
      "Epoch 04: train_acc=0.4814, val_acc=0.5641, train_loss=2.2738, val_loss=1.9102\n",
      "Epoch 05: train_acc=0.5487, val_acc=0.6121, train_loss=1.9397, val_loss=1.6334\n",
      "Epoch 06: train_acc=0.6255, val_acc=0.6922, train_loss=1.5827, val_loss=1.3089\n",
      "Epoch 07: train_acc=0.6611, val_acc=0.7251, train_loss=1.4212, val_loss=1.1691\n",
      "Epoch 08: train_acc=0.6901, val_acc=0.7471, train_loss=1.2916, val_loss=1.0680\n",
      "Epoch 09: train_acc=0.7117, val_acc=0.7503, train_loss=1.1840, val_loss=1.0529\n",
      "Epoch 10: train_acc=0.7356, val_acc=0.7630, train_loss=1.0859, val_loss=0.9793\n"
     ]
    }
   ],
   "source": [
    "if best_state_dict is not None:\n",
    "    torch.save(best_state_dict, \"best_custom_cnn_image_custom.pth\")\n",
    "    print(\"Saved best model -> best_custom_cnn_image_custom.pth\")\n",
    "\n",
    "with open(\"class_to_idx_image_custom.json\", \"w\") as f:\n",
    "    json.dump(class_to_idx, f, indent=2)\n",
    "print(\"Saved class_to_idx_image_custom.json\")\n",
    "\n",
    "print(\"\\nTraining curves:\")\n",
    "for i in range(EPOCHS):\n",
    "    print(\n",
    "        f\"Epoch {i+1:02d}: \"\n",
    "        f\"train_acc={history['train_acc'][i]:.4f}, \"\n",
    "        f\"val_acc={history['val_acc'][i]:.4f}, \"\n",
    "        f\"train_loss={history['train_loss'][i]:.4f}, \"\n",
    "        f\"val_loss={history['val_loss'][i]:.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc1c535-c8f1-45ba-a833-3a97daacb98e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (GPU)",
   "language": "python",
   "name": "my_gpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
