{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a7ab48e-5b1a-4593-87d4-ef4bc3251d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.optim import Adam\n",
    "#from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94856d52-c955-485d-b915-2665887175bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw columns: ['subject', 'body', 'label', 'sender', 'receiver', 'date', 'urls', 'source', 'subject_clean', 'body_clean', 'subject_length', 'body_length']\n",
      "                                             subject  \\\n",
      "0                          Never agree to be a loser   \n",
      "1                             Befriend Jenna Jameson   \n",
      "2                               CNN.com Daily Top 10   \n",
      "3  Re: svn commit: r619753 - in /spamassassin/tru...   \n",
      "4                         SpecialPricesPharmMoreinfo   \n",
      "\n",
      "                                                body  label  \\\n",
      "0  Buck up, your troubles caused by small dimensi...      1   \n",
      "1  \\nUpgrade your sex and pleasures with these te...      1   \n",
      "2  >+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+...      1   \n",
      "3  Would anyone object to removing .so from this ...      0   \n",
      "4  \\nWelcomeFastShippingCustomerSupport\\nhttp://7...      1   \n",
      "\n",
      "                                              sender  \\\n",
      "0                   Young Esposito <Young@iworld.de>   \n",
      "1                       Mok <ipline's1983@icable.ph>   \n",
      "2  Daily Top 10 <Karmandeep-opengevl@universalnet...   \n",
      "3                 Michael Parker <ivqrnai@pobox.com>   \n",
      "4  Gretchen Suggs <externalsep1@loanofficertool.com>   \n",
      "\n",
      "                                         receiver                       date  \\\n",
      "0                     user4@gvc.ceas-challenge.cc  2008-08-05 16:31:02-07:00   \n",
      "1                   user2.2@gvc.ceas-challenge.cc  2008-08-05 18:31:03-05:00   \n",
      "2                   user2.9@gvc.ceas-challenge.cc  2008-08-05 20:28:00-12:00   \n",
      "3  SpamAssassin Dev <xrh@spamassassin.apache.org>  2008-08-05 17:31:20-06:00   \n",
      "4                   user2.2@gvc.ceas-challenge.cc  2008-08-05 19:31:21-04:00   \n",
      "\n",
      "   urls   source                                      subject_clean  \\\n",
      "0   1.0  CEAS_08                          never agree to be a loser   \n",
      "1   1.0  CEAS_08                             befriend jenna jameson   \n",
      "2   1.0  CEAS_08                               cnn.com daily top 10   \n",
      "3   1.0  CEAS_08  re: svn commit: r619753 - in /spamassassin/tru...   \n",
      "4   1.0  CEAS_08                         specialpricespharmmoreinfo   \n",
      "\n",
      "                                          body_clean  subject_length  \\\n",
      "0  buck up, your troubles caused by small dimensi...              25   \n",
      "1  upgrade your sex and pleasures with these tech...              22   \n",
      "2  >+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+...              20   \n",
      "3  would anyone object to removing .so from this ...             150   \n",
      "4  welcomefastshippingcustomersupport http://7iwf...              26   \n",
      "\n",
      "   body_length  \n",
      "0          273  \n",
      "1           82  \n",
      "2         3918  \n",
      "3        24418  \n",
      "4          175  \n",
      "Dataset size: 76346\n",
      "                                                text  label\n",
      "0  Never agree to be a loser Buck up, your troubl...      1\n",
      "1  Befriend Jenna Jameson \\nUpgrade your sex and ...      1\n",
      "2  CNN.com Daily Top 10 >+=+=+=+=+=+=+=+=+=+=+=+=...      1\n",
      "3  Re: svn commit: r619753 - in /spamassassin/tru...      0\n",
      "4  SpecialPricesPharmMoreinfo \\nWelcomeFastShippi...      1\n"
     ]
    }
   ],
   "source": [
    "CSV_PATH = \"cleaned_combined_emails.csv\"\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "print(\"Raw columns:\", df.columns.tolist())\n",
    "print(df.head())\n",
    "\n",
    "# Keep only what we need\n",
    "df = df[[\"subject\", \"body\", \"label\"]]\n",
    "\n",
    "# Drop rows with missing text or label\n",
    "df = df.dropna(subset=[\"subject\", \"body\", \"label\"])\n",
    "\n",
    "# Combine subject + body into one text field\n",
    "df[\"text\"] = df[\"subject\"].astype(str) + \" \" + df[\"body\"].astype(str)\n",
    "\n",
    "# Make sure labels are ints (0 or 1)\n",
    "df[\"label\"] = df[\"label\"].astype(int)\n",
    "\n",
    "print(\"Dataset size:\", len(df))\n",
    "print(df[[\"text\", \"label\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c5dd7a8-16da-40e6-9a36-d8d01692328b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 61076\n",
      "Val size: 15270\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>body</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>36422</th>\n",
       "      <td>CNN Alerts: My Custom Alert</td>\n",
       "      <td>\\n\\nCNN Alerts: My Custom Alert\\n\\n\\n\\n\\n\\n\\n ...</td>\n",
       "      <td>1</td>\n",
       "      <td>CNN Alerts: My Custom Alert \\n\\nCNN Alerts: My...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6494</th>\n",
       "      <td>You won't believe! It's incredible!</td>\n",
       "      <td>Don't treat your small dimension as a can of w...</td>\n",
       "      <td>1</td>\n",
       "      <td>You won't believe! It's incredible! Don't trea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30441</th>\n",
       "      <td>typeset wineskin teach miterwort beloit</td>\n",
       "      <td>\\ndesign tonnage wattle? beloit, wattle malice...</td>\n",
       "      <td>1</td>\n",
       "      <td>typeset wineskin teach miterwort beloit \\ndesi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60041</th>\n",
       "      <td>fda approved drug lasts 8 x longer than vi - a...</td>\n",
       "      <td>body bgcolor = blacktable cellpadding = 10 bor...</td>\n",
       "      <td>1</td>\n",
       "      <td>fda approved drug lasts 8 x longer than vi - a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3378</th>\n",
       "      <td>Re: [Python-3000] Python 2.6 and 3.0</td>\n",
       "      <td>Barry Warsaw wrote:\\n&gt; From the follow ups, it...</td>\n",
       "      <td>0</td>\n",
       "      <td>Re: [Python-3000] Python 2.6 and 3.0 Barry War...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 subject  \\\n",
       "36422                        CNN Alerts: My Custom Alert   \n",
       "6494                 You won't believe! It's incredible!   \n",
       "30441            typeset wineskin teach miterwort beloit   \n",
       "60041  fda approved drug lasts 8 x longer than vi - a...   \n",
       "3378                Re: [Python-3000] Python 2.6 and 3.0   \n",
       "\n",
       "                                                    body  label  \\\n",
       "36422  \\n\\nCNN Alerts: My Custom Alert\\n\\n\\n\\n\\n\\n\\n ...      1   \n",
       "6494   Don't treat your small dimension as a can of w...      1   \n",
       "30441  \\ndesign tonnage wattle? beloit, wattle malice...      1   \n",
       "60041  body bgcolor = blacktable cellpadding = 10 bor...      1   \n",
       "3378   Barry Warsaw wrote:\\n> From the follow ups, it...      0   \n",
       "\n",
       "                                                    text  \n",
       "36422  CNN Alerts: My Custom Alert \\n\\nCNN Alerts: My...  \n",
       "6494   You won't believe! It's incredible! Don't trea...  \n",
       "30441  typeset wineskin teach miterwort beloit \\ndesi...  \n",
       "60041  fda approved drug lasts 8 x longer than vi - a...  \n",
       "3378   Re: [Python-3000] Python 2.6 and 3.0 Barry War...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df, val_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=df[\"label\"]\n",
    ")\n",
    "\n",
    "print(\"Train size:\", len(train_df))\n",
    "print(\"Val size:\", len(val_df))\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d47309c0-caa2-4240-b404-fc7aa9e86430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 207037\n",
      "Sample vocab items: [('<PAD>', 0), ('<UNK>', 1), ('-', 2), ('--', 3), ('---', 4), ('----', 5), ('-----', 6), ('------', 7), ('-------', 8), ('--------', 9), ('---------', 10), ('----------', 11), ('-----------', 12), ('------------', 13), ('-------------', 14), ('--------------', 15), ('---------------', 16), ('----------------', 17), ('-----------------', 18), ('------------------', 19)]\n"
     ]
    }
   ],
   "source": [
    "def simple_tokenize(text):\n",
    "    \"\"\"\n",
    "    Basic tokenizer: lowercase and split on whitespace after removing junk.\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z0-9@.\\-_/ ]+\", \" \", text)\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# Build vocab from training set only\n",
    "word_freq = {}\n",
    "for t in train_df[\"text\"]:\n",
    "    for tok in simple_tokenize(t):\n",
    "        word_freq[tok] = word_freq.get(tok, 0) + 1\n",
    "\n",
    "# Drop super-rare words\n",
    "min_freq = 2\n",
    "vocab_words = [w for w, f in word_freq.items() if f >= min_freq]\n",
    "\n",
    "# Reserve 0 for PAD, 1 for UNK\n",
    "itos = [\"<PAD>\", \"<UNK>\"] + sorted(vocab_words)\n",
    "stoi = {w: i for i, w in enumerate(itos)}\n",
    "\n",
    "vocab_size = len(itos)\n",
    "print(\"Vocab size:\", vocab_size)\n",
    "print(\"Sample vocab items:\", list(stoi.items())[:20])\n",
    "\n",
    "\n",
    "def numericalize(tokens, stoi_map, unk_idx=1):\n",
    "    return [stoi_map.get(tok, unk_idx) for tok in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d56fb7d-e64f-46c1-83e7-00b53ddf1119",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmailDataset(Dataset):\n",
    "    def __init__(self, df, stoi_map):\n",
    "        self.texts = df[\"text\"].tolist()\n",
    "        self.labels = df[\"label\"].tolist()\n",
    "        self.stoi_map = stoi_map\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        raw_text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        toks = simple_tokenize(raw_text)\n",
    "        ids = numericalize(toks, self.stoi_map)\n",
    "\n",
    "        return torch.tensor(ids, dtype=torch.long), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "\n",
    "def collate_batch(batch):\n",
    "    \"\"\"\n",
    "    Pad sequences to max length in batch.\n",
    "    \"\"\"\n",
    "    (seqs, labels) = zip(*batch)\n",
    "    padded_seqs = pad_sequence(seqs, batch_first=True, padding_value=0)\n",
    "    labels = torch.stack(labels)\n",
    "    return padded_seqs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "542d442a-0a00-4179-8bbf-c8e888e66054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DataLoader Configuration\n",
      "============================================================\n",
      "Batch size: 32\n",
      "Max sequence length: 512\n",
      "Train batches: 1909\n",
      "Val batches: 478\n",
      "\n",
      "Sample batch:\n",
      "  Batch X shape: torch.Size([32, 512])\n",
      "  Batch y shape: torch.Size([32])\n",
      "  Actual max length in batch: 512\n",
      "  First 50 token ids: tensor([ 81759, 183826,  24811,   2243,   9121, 199838, 142545, 188832,  34857,\n",
      "        187011, 136316,  25341,      2,  12948, 188832,  66832,  29887,   1558,\n",
      "        199838, 102402, 178164, 183826,  94584,  90970, 134612, 187094, 199838,\n",
      "        142545, 188832,  37588,   1558, 187072, 135521,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0])\n",
      "  First label: 0\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQ_LENGTH = 512  # Maximum tokens per email (adjust if needed)\n",
    "\n",
    "class EmailDataset(Dataset):\n",
    "    def __init__(self, df, stoi_map, max_length=512):\n",
    "        self.texts = df[\"text\"].tolist()\n",
    "        self.labels = df[\"label\"].tolist()\n",
    "        self.stoi_map = stoi_map\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        raw_text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        toks = simple_tokenize(raw_text)\n",
    "        \n",
    "        # TRUNCATE to max_length to prevent OOM\n",
    "        if len(toks) > self.max_length:\n",
    "            toks = toks[:self.max_length]\n",
    "        \n",
    "        ids = numericalize(toks, self.stoi_map)\n",
    "\n",
    "        return torch.tensor(ids, dtype=torch.long), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "\n",
    "def collate_batch(batch):\n",
    "    \"\"\"\n",
    "    Pad sequences to max length in batch.\n",
    "    \"\"\"\n",
    "    (seqs, labels) = zip(*batch)\n",
    "    padded_seqs = pad_sequence(seqs, batch_first=True, padding_value=0)\n",
    "    labels = torch.stack(labels)\n",
    "    return padded_seqs, labels\n",
    "\n",
    "\n",
    "# Create datasets with max_length limit\n",
    "train_dataset = EmailDataset(train_df, stoi, max_length=MAX_SEQ_LENGTH)\n",
    "val_dataset = EmailDataset(val_df, stoi, max_length=MAX_SEQ_LENGTH)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_batch\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_batch\n",
    ")\n",
    "\n",
    "# Sanity check\n",
    "batch_x, batch_y = next(iter(train_loader))\n",
    "print(\"=\"*60)\n",
    "print(\"DataLoader Configuration\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Max sequence length: {MAX_SEQ_LENGTH}\")\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")\n",
    "print(f\"\\nSample batch:\")\n",
    "print(f\"  Batch X shape: {batch_x.shape}\")  # Should be [32, <=512]\n",
    "print(f\"  Batch y shape: {batch_y.shape}\")  # Should be [32]\n",
    "print(f\"  Actual max length in batch: {batch_x.shape[1]}\")\n",
    "print(f\"  First 50 token ids: {batch_x[0][:50]}\")\n",
    "print(f\"  First label: {batch_y[0]}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32655eb8-63e4-4b2e-9dd3-b33bf1a74593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomCNN_Text(\n",
      "  (embedding): Embedding(207037, 128, padding_idx=0)\n",
      "  (conv1): Conv1d(128, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu1): ReLU(inplace=True)\n",
      "  (pool1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu2): ReLU(inplace=True)\n",
      "  (pool2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv1d(128, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (bn3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu3): ReLU(inplace=True)\n",
      "  (pool3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv4): Conv1d(256, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (bn4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu4): ReLU(inplace=True)\n",
      "  (pool4): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (global_pool): AdaptiveAvgPool1d(output_size=1)\n",
      "  (fc1): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (relu_fc1): ReLU(inplace=True)\n",
      "  (dropout1): Dropout(p=0.5, inplace=False)\n",
      "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (relu_fc2): ReLU(inplace=True)\n",
      "  (dropout2): Dropout(p=0.5, inplace=False)\n",
      "  (fc3): Linear(in_features=128, out_features=2, bias=True)\n",
      ")\n",
      "\n",
      "Total parameters: 27,208,770\n",
      "Trainable parameters: 27,208,770\n"
     ]
    }
   ],
   "source": [
    "class CustomCNN_Text(nn.Module):\n",
    "    \"\"\"\n",
    "    Custom CNN for text classification with medium depth.\n",
    "    Architecture: Embedding + 4 Conv1d blocks + 3 FC layers\n",
    "    MATCHES the structure of CustomCNN_Image but with Conv1d instead of Conv2d\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embed_dim=128, num_classes=2, dropout=0.5):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        \n",
    "        # Block 1: embed_dim -> 64 channels\n",
    "        self.conv1 = nn.Conv1d(embed_dim, 64, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Block 2: 64 -> 128 channels\n",
    "        self.conv2 = nn.Conv1d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Block 3: 128 -> 256 channels\n",
    "        self.conv3 = nn.Conv1d(128, 256, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm1d(256)\n",
    "        self.relu3 = nn.ReLU(inplace=True)\n",
    "        self.pool3 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Block 4: 256 -> 512 channels\n",
    "        self.conv4 = nn.Conv1d(256, 512, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm1d(512)\n",
    "        self.relu4 = nn.ReLU(inplace=True)\n",
    "        self.pool4 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Global Average Pooling (adaptive pooling to fixed size)\n",
    "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        \n",
    "        # Fully Connected Layers (SAME as image model)\n",
    "        self.fc1 = nn.Linear(512, 256)\n",
    "        self.relu_fc1 = nn.ReLU(inplace=True)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        \n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.relu_fc2 = nn.ReLU(inplace=True)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        \n",
    "        self.fc3 = nn.Linear(128, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Embedding: [B, T] -> [B, T, E]\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # Transpose for Conv1d: [B, T, E] -> [B, E, T]\n",
    "        x = x.transpose(1, 2)\n",
    "        \n",
    "        # Block 1\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        # Block 2\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        # Block 3\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.pool3(x)\n",
    "        \n",
    "        # Block 4\n",
    "        x = self.conv4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.pool4(x)\n",
    "        \n",
    "        # Global pooling: [B, 512, T] -> [B, 512, 1]\n",
    "        x = self.global_pool(x)\n",
    "        x = x.squeeze(-1)  # [B, 512]\n",
    "        \n",
    "        # FC layers\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu_fc1(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = self.relu_fc2(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "# Initialize model\n",
    "num_classes = df[\"label\"].nunique()\n",
    "model = CustomCNN_Text(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_dim=128,\n",
    "    num_classes=num_classes,\n",
    "    dropout=0.5\n",
    ").to(device)\n",
    "\n",
    "print(model)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5833bf02-1f68-4701-bdfe-d86014b4fdae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Criterion: CrossEntropyLoss()\n",
      "Optimizer: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "print(\"Criterion:\", criterion)\n",
    "print(\"Optimizer:\", optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc623a2b-20d4-4de2-a9b6-ecfb65f67832",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_count = 0\n",
    "\n",
    "    for batch_x, batch_y in tqdm(loader, desc=\"Train\", leave=False):\n",
    "        batch_x = batch_x.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits = model(batch_x)\n",
    "        loss = criterion(logits, batch_y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * batch_x.size(0)\n",
    "\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        total_correct += (preds == batch_y).sum().item()\n",
    "        total_count += batch_x.size(0)\n",
    "\n",
    "    avg_loss = total_loss / total_count\n",
    "    avg_acc = total_correct / total_count\n",
    "    return avg_loss, avg_acc\n",
    "\n",
    "\n",
    "def eval_one_epoch(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in tqdm(loader, desc=\"Val\", leave=False):\n",
    "            batch_x = batch_x.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "\n",
    "            logits = model(batch_x)\n",
    "            loss = criterion(logits, batch_y)\n",
    "\n",
    "            total_loss += loss.item() * batch_x.size(0)\n",
    "\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            total_correct += (preds == batch_y).sum().item()\n",
    "            total_count += batch_x.size(0)\n",
    "\n",
    "    avg_loss = total_loss / total_count\n",
    "    avg_acc = total_correct / total_count\n",
    "    return avg_loss, avg_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "324d7a5d-5757-41cd-aebd-806c91608709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "GPU Memory Status\n",
      "============================================================\n",
      "Device: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "Total Memory: 8.59 GB\n",
      "Allocated: 0.11 GB\n",
      "Cached: 0.13 GB\n",
      "Available: 8.48 GB\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Clear GPU memory before training\n",
    "import gc\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"GPU Memory Status\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Device: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"Total Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "print(f\"Allocated: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")\n",
    "print(f\"Cached: {torch.cuda.memory_reserved(0) / 1e9:.2f} GB\")\n",
    "print(f\"Available: {(torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated(0)) / 1e9:.2f} GB\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab01e46f-f9ee-42f4-ba58-0df3693c454c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for 10 epochs...\n",
      "Total training batches: 1909\n",
      "Total validation batches: 478\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "Epoch 1/10\n",
      "======================================================================\n",
      "[TRAINING] Processing 1909 batches...\n",
      "  [ 10.0%] Batch  190/1909 | Loss: 0.0003 | Acc: 0.9931\n",
      "  [ 19.9%] Batch  380/1909 | Loss: 0.0220 | Acc: 0.9938\n",
      "  [ 29.9%] Batch  570/1909 | Loss: 0.0201 | Acc: 0.9940\n",
      "  [ 39.8%] Batch  760/1909 | Loss: 0.0178 | Acc: 0.9939\n",
      "  [ 49.8%] Batch  950/1909 | Loss: 0.0009 | Acc: 0.9934\n",
      "  [ 59.7%] Batch 1140/1909 | Loss: 0.0050 | Acc: 0.9937\n",
      "  [ 69.7%] Batch 1330/1909 | Loss: 0.0061 | Acc: 0.9938\n",
      "  [ 79.6%] Batch 1520/1909 | Loss: 0.0006 | Acc: 0.9939\n",
      "  [ 89.6%] Batch 1710/1909 | Loss: 0.0035 | Acc: 0.9939\n",
      "  [ 99.5%] Batch 1900/1909 | Loss: 0.0083 | Acc: 0.9939\n",
      "  [100.0%] Batch 1909/1909 | Loss: 0.0029 | Acc: 0.9939\n",
      "\n",
      "[VALIDATION] Processing 478 batches...\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "[Epoch 1/10 Summary]\n",
      "  Train Loss: 0.0203 | Train Acc: 0.9939 (99.39%)\n",
      "  Val Loss:   0.1220   | Val Acc:   0.9804 (98.04%)\n",
      "  New BEST model! (val_acc=0.9804)\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "======================================================================\n",
      "Epoch 2/10\n",
      "======================================================================\n",
      "[TRAINING] Processing 1909 batches...\n",
      "  [ 10.0%] Batch  190/1909 | Loss: 0.0668 | Acc: 0.9961\n",
      "  [ 19.9%] Batch  380/1909 | Loss: 0.0002 | Acc: 0.9969\n",
      "  [ 29.9%] Batch  570/1909 | Loss: 0.0001 | Acc: 0.9967\n",
      "  [ 39.8%] Batch  760/1909 | Loss: 0.0053 | Acc: 0.9969\n",
      "  [ 49.8%] Batch  950/1909 | Loss: 0.0001 | Acc: 0.9966\n",
      "  [ 59.7%] Batch 1140/1909 | Loss: 0.0312 | Acc: 0.9967\n",
      "  [ 69.7%] Batch 1330/1909 | Loss: 0.0006 | Acc: 0.9969\n",
      "  [ 79.6%] Batch 1520/1909 | Loss: 0.0001 | Acc: 0.9968\n",
      "  [ 89.6%] Batch 1710/1909 | Loss: 0.0076 | Acc: 0.9968\n",
      "  [ 99.5%] Batch 1900/1909 | Loss: 0.0022 | Acc: 0.9968\n",
      "  [100.0%] Batch 1909/1909 | Loss: 0.0011 | Acc: 0.9968\n",
      "\n",
      "[VALIDATION] Processing 478 batches...\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "[Epoch 2/10 Summary]\n",
      "  Train Loss: 0.0114 | Train Acc: 0.9968 (99.68%)\n",
      "  Val Loss:   0.0511   | Val Acc:   0.9889 (98.89%)\n",
      "  New BEST model! (val_acc=0.9889)\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "======================================================================\n",
      "Epoch 3/10\n",
      "======================================================================\n",
      "[TRAINING] Processing 1909 batches...\n",
      "  [ 10.0%] Batch  190/1909 | Loss: 0.0005 | Acc: 0.9985\n",
      "  [ 19.9%] Batch  380/1909 | Loss: 0.0003 | Acc: 0.9979\n",
      "  [ 29.9%] Batch  570/1909 | Loss: 0.0017 | Acc: 0.9978\n",
      "  [ 39.8%] Batch  760/1909 | Loss: 0.0380 | Acc: 0.9978\n",
      "  [ 49.8%] Batch  950/1909 | Loss: 0.0002 | Acc: 0.9979\n",
      "  [ 59.7%] Batch 1140/1909 | Loss: 0.0000 | Acc: 0.9978\n",
      "  [ 69.7%] Batch 1330/1909 | Loss: 0.0102 | Acc: 0.9979\n",
      "  [ 79.6%] Batch 1520/1909 | Loss: 0.0018 | Acc: 0.9978\n",
      "  [ 89.6%] Batch 1710/1909 | Loss: 0.0002 | Acc: 0.9979\n",
      "  [ 99.5%] Batch 1900/1909 | Loss: 0.0002 | Acc: 0.9978\n",
      "  [100.0%] Batch 1909/1909 | Loss: 0.0001 | Acc: 0.9978\n",
      "\n",
      "[VALIDATION] Processing 478 batches...\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "[Epoch 3/10 Summary]\n",
      "  Train Loss: 0.0078 | Train Acc: 0.9978 (99.78%)\n",
      "  Val Loss:   0.0752   | Val Acc:   0.9887 (98.87%)\n",
      "  (Best val_acc so far: 0.9889)\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "======================================================================\n",
      "Epoch 4/10\n",
      "======================================================================\n",
      "[TRAINING] Processing 1909 batches...\n",
      "  [ 10.0%] Batch  190/1909 | Loss: 0.0506 | Acc: 0.9977\n",
      "  [ 19.9%] Batch  380/1909 | Loss: 0.0000 | Acc: 0.9988\n",
      "  [ 29.9%] Batch  570/1909 | Loss: 0.0001 | Acc: 0.9986\n",
      "  [ 39.8%] Batch  760/1909 | Loss: 0.0006 | Acc: 0.9982\n",
      "  [ 49.8%] Batch  950/1909 | Loss: 0.0000 | Acc: 0.9984\n",
      "  [ 59.7%] Batch 1140/1909 | Loss: 0.3632 | Acc: 0.9982\n",
      "  [ 69.7%] Batch 1330/1909 | Loss: 0.0202 | Acc: 0.9981\n",
      "  [ 79.6%] Batch 1520/1909 | Loss: 0.0001 | Acc: 0.9982\n",
      "  [ 89.6%] Batch 1710/1909 | Loss: 0.0082 | Acc: 0.9982\n",
      "  [ 99.5%] Batch 1900/1909 | Loss: 0.0001 | Acc: 0.9983\n",
      "  [100.0%] Batch 1909/1909 | Loss: 0.0002 | Acc: 0.9983\n",
      "\n",
      "[VALIDATION] Processing 478 batches...\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "[Epoch 4/10 Summary]\n",
      "  Train Loss: 0.0079 | Train Acc: 0.9983 (99.83%)\n",
      "  Val Loss:   0.0653   | Val Acc:   0.9881 (98.81%)\n",
      "  (Best val_acc so far: 0.9889)\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "======================================================================\n",
      "Epoch 5/10\n",
      "======================================================================\n",
      "[TRAINING] Processing 1909 batches...\n",
      "  [ 10.0%] Batch  190/1909 | Loss: 0.0000 | Acc: 0.9993\n",
      "  [ 19.9%] Batch  380/1909 | Loss: 0.0001 | Acc: 0.9992\n",
      "  [ 29.9%] Batch  570/1909 | Loss: 0.0163 | Acc: 0.9987\n",
      "  [ 39.8%] Batch  760/1909 | Loss: 0.0000 | Acc: 0.9988\n",
      "  [ 49.8%] Batch  950/1909 | Loss: 0.0020 | Acc: 0.9986\n",
      "  [ 59.7%] Batch 1140/1909 | Loss: 0.0007 | Acc: 0.9987\n",
      "  [ 69.7%] Batch 1330/1909 | Loss: 0.0000 | Acc: 0.9987\n",
      "  [ 79.6%] Batch 1520/1909 | Loss: 0.0009 | Acc: 0.9988\n",
      "  [ 89.6%] Batch 1710/1909 | Loss: 0.0001 | Acc: 0.9988\n",
      "  [ 99.5%] Batch 1900/1909 | Loss: 0.0001 | Acc: 0.9988\n",
      "  [100.0%] Batch 1909/1909 | Loss: 0.0003 | Acc: 0.9988\n",
      "\n",
      "[VALIDATION] Processing 478 batches...\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "[Epoch 5/10 Summary]\n",
      "  Train Loss: 0.0042 | Train Acc: 0.9988 (99.88%)\n",
      "  Val Loss:   0.1015   | Val Acc:   0.9869 (98.69%)\n",
      "  (Best val_acc so far: 0.9889)\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "======================================================================\n",
      "Epoch 6/10\n",
      "======================================================================\n",
      "[TRAINING] Processing 1909 batches...\n",
      "  [ 10.0%] Batch  190/1909 | Loss: 0.0033 | Acc: 0.9992\n",
      "  [ 19.9%] Batch  380/1909 | Loss: 0.0008 | Acc: 0.9992\n",
      "  [ 29.9%] Batch  570/1909 | Loss: 0.0000 | Acc: 0.9992\n",
      "  [ 39.8%] Batch  760/1909 | Loss: 0.0000 | Acc: 0.9993\n",
      "  [ 49.8%] Batch  950/1909 | Loss: 0.0002 | Acc: 0.9992\n",
      "  [ 59.7%] Batch 1140/1909 | Loss: 0.0001 | Acc: 0.9992\n",
      "  [ 69.7%] Batch 1330/1909 | Loss: 0.0019 | Acc: 0.9992\n",
      "  [ 79.6%] Batch 1520/1909 | Loss: 0.0000 | Acc: 0.9992\n",
      "  [ 89.6%] Batch 1710/1909 | Loss: 0.0044 | Acc: 0.9991\n",
      "  [ 99.5%] Batch 1900/1909 | Loss: 0.0003 | Acc: 0.9990\n",
      "  [100.0%] Batch 1909/1909 | Loss: 0.0000 | Acc: 0.9990\n",
      "\n",
      "[VALIDATION] Processing 478 batches...\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "[Epoch 6/10 Summary]\n",
      "  Train Loss: 0.0047 | Train Acc: 0.9990 (99.90%)\n",
      "  Val Loss:   0.1731   | Val Acc:   0.9823 (98.23%)\n",
      "  (Best val_acc so far: 0.9889)\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "======================================================================\n",
      "Epoch 7/10\n",
      "======================================================================\n",
      "[TRAINING] Processing 1909 batches...\n",
      "  [ 10.0%] Batch  190/1909 | Loss: 0.0000 | Acc: 0.9977\n",
      "  [ 19.9%] Batch  380/1909 | Loss: 0.0002 | Acc: 0.9985\n",
      "  [ 29.9%] Batch  570/1909 | Loss: 0.0016 | Acc: 0.9978\n",
      "  [ 39.8%] Batch  760/1909 | Loss: 0.0246 | Acc: 0.9979\n",
      "  [ 49.8%] Batch  950/1909 | Loss: 0.0007 | Acc: 0.9981\n",
      "  [ 59.7%] Batch 1140/1909 | Loss: 0.0000 | Acc: 0.9982\n",
      "  [ 69.7%] Batch 1330/1909 | Loss: 0.0011 | Acc: 0.9983\n",
      "  [ 79.6%] Batch 1520/1909 | Loss: 0.0000 | Acc: 0.9985\n",
      "  [ 89.6%] Batch 1710/1909 | Loss: 0.0000 | Acc: 0.9985\n",
      "  [ 99.5%] Batch 1900/1909 | Loss: 0.0000 | Acc: 0.9986\n",
      "  [100.0%] Batch 1909/1909 | Loss: 0.0001 | Acc: 0.9986\n",
      "\n",
      "[VALIDATION] Processing 478 batches...\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "[Epoch 7/10 Summary]\n",
      "  Train Loss: 0.0064 | Train Acc: 0.9986 (99.86%)\n",
      "  Val Loss:   0.0706   | Val Acc:   0.9892 (98.92%)\n",
      "  New BEST model! (val_acc=0.9892)\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "======================================================================\n",
      "Epoch 8/10\n",
      "======================================================================\n",
      "[TRAINING] Processing 1909 batches...\n",
      "  [ 10.0%] Batch  190/1909 | Loss: 0.0000 | Acc: 0.9995\n",
      "  [ 19.9%] Batch  380/1909 | Loss: 0.0000 | Acc: 0.9997\n",
      "  [ 29.9%] Batch  570/1909 | Loss: 0.0000 | Acc: 0.9998\n",
      "  [ 39.8%] Batch  760/1909 | Loss: 0.0000 | Acc: 0.9998\n",
      "  [ 49.8%] Batch  950/1909 | Loss: 0.0001 | Acc: 0.9996\n",
      "  [ 59.7%] Batch 1140/1909 | Loss: 0.0001 | Acc: 0.9995\n",
      "  [ 69.7%] Batch 1330/1909 | Loss: 0.0004 | Acc: 0.9994\n",
      "  [ 79.6%] Batch 1520/1909 | Loss: 0.0000 | Acc: 0.9994\n",
      "  [ 89.6%] Batch 1710/1909 | Loss: 0.0000 | Acc: 0.9994\n",
      "  [ 99.5%] Batch 1900/1909 | Loss: 0.0000 | Acc: 0.9992\n",
      "  [100.0%] Batch 1909/1909 | Loss: 0.0000 | Acc: 0.9992\n",
      "\n",
      "[VALIDATION] Processing 478 batches...\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "[Epoch 8/10 Summary]\n",
      "  Train Loss: 0.0032 | Train Acc: 0.9992 (99.92%)\n",
      "  Val Loss:   0.1194   | Val Acc:   0.9874 (98.74%)\n",
      "  (Best val_acc so far: 0.9892)\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "======================================================================\n",
      "Epoch 9/10\n",
      "======================================================================\n",
      "[TRAINING] Processing 1909 batches...\n",
      "  [ 10.0%] Batch  190/1909 | Loss: 0.0000 | Acc: 0.9995\n",
      "  [ 19.9%] Batch  380/1909 | Loss: 0.0000 | Acc: 0.9998\n",
      "  [ 29.9%] Batch  570/1909 | Loss: 0.0000 | Acc: 0.9995\n",
      "  [ 39.8%] Batch  760/1909 | Loss: 0.0000 | Acc: 0.9993\n",
      "  [ 49.8%] Batch  950/1909 | Loss: 0.0000 | Acc: 0.9993\n",
      "  [ 59.7%] Batch 1140/1909 | Loss: 0.0000 | Acc: 0.9992\n",
      "  [ 69.7%] Batch 1330/1909 | Loss: 0.0000 | Acc: 0.9992\n",
      "  [ 79.6%] Batch 1520/1909 | Loss: 0.0000 | Acc: 0.9993\n",
      "  [ 89.6%] Batch 1710/1909 | Loss: 0.0000 | Acc: 0.9993\n",
      "  [ 99.5%] Batch 1900/1909 | Loss: 0.0002 | Acc: 0.9993\n",
      "  [100.0%] Batch 1909/1909 | Loss: 0.0000 | Acc: 0.9993\n",
      "\n",
      "[VALIDATION] Processing 478 batches...\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "[Epoch 9/10 Summary]\n",
      "  Train Loss: 0.0029 | Train Acc: 0.9993 (99.93%)\n",
      "  Val Loss:   0.0881   | Val Acc:   0.9892 (98.92%)\n",
      "  (Best val_acc so far: 0.9892)\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "======================================================================\n",
      "Epoch 10/10\n",
      "======================================================================\n",
      "[TRAINING] Processing 1909 batches...\n",
      "  [ 10.0%] Batch  190/1909 | Loss: 0.0000 | Acc: 0.9995\n",
      "  [ 19.9%] Batch  380/1909 | Loss: 0.0001 | Acc: 0.9997\n",
      "  [ 29.9%] Batch  570/1909 | Loss: 0.0001 | Acc: 0.9996\n",
      "  [ 39.8%] Batch  760/1909 | Loss: 0.0000 | Acc: 0.9997\n",
      "  [ 49.8%] Batch  950/1909 | Loss: 0.0000 | Acc: 0.9998\n",
      "  [ 59.7%] Batch 1140/1909 | Loss: 0.0000 | Acc: 0.9998\n",
      "  [ 69.7%] Batch 1330/1909 | Loss: 0.0006 | Acc: 0.9995\n",
      "  [ 79.6%] Batch 1520/1909 | Loss: 0.0012 | Acc: 0.9994\n",
      "  [ 89.6%] Batch 1710/1909 | Loss: 0.0000 | Acc: 0.9995\n",
      "  [ 99.5%] Batch 1900/1909 | Loss: 0.0000 | Acc: 0.9994\n",
      "  [100.0%] Batch 1909/1909 | Loss: 0.0000 | Acc: 0.9993\n",
      "\n",
      "[VALIDATION] Processing 478 batches...\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "[Epoch 10/10 Summary]\n",
      "  Train Loss: 0.0035 | Train Acc: 0.9993 (99.93%)\n",
      "  Val Loss:   0.1059   | Val Acc:   0.9896 (98.96%)\n",
      "  New BEST model! (val_acc=0.9896)\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "======================================================================\n",
      "=== Training Complete ===\n",
      "======================================================================\n",
      "Best Validation Accuracy: 0.9896 (98.96%)\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "best_val_acc = 0.0\n",
    "best_state_dict = None\n",
    "\n",
    "history = {\n",
    "    \"train_loss\": [],\n",
    "    \"train_acc\": [],\n",
    "    \"val_loss\": [],\n",
    "    \"val_acc\": []\n",
    "}\n",
    "\n",
    "print(f\"Starting training for {EPOCHS} epochs...\")\n",
    "print(f\"Total training batches: {len(train_loader)}\")\n",
    "print(f\"Total validation batches: {len(val_loader)}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # ============================================\n",
    "    # TRAINING PHASE\n",
    "    # ============================================\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_correct = 0\n",
    "    running_total = 0\n",
    "    \n",
    "    total_train_batches = len(train_loader)\n",
    "    print_every = max(1, total_train_batches // 10)  # Print 10 times per epoch\n",
    "    \n",
    "    print(f\"[TRAINING] Processing {total_train_batches} batches...\")\n",
    "    \n",
    "    for batch_idx, (batch_x, batch_y) in enumerate(train_loader):\n",
    "        batch_x = batch_x.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits = model(batch_x)\n",
    "        loss = criterion(logits, batch_y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * batch_x.size(0)\n",
    "\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        running_correct += (preds == batch_y).sum().item()\n",
    "        running_total += batch_x.size(0)\n",
    "        \n",
    "        # Print progress\n",
    "        if (batch_idx + 1) % print_every == 0 or (batch_idx + 1) == total_train_batches:\n",
    "            current_acc = running_correct / running_total\n",
    "            progress = (batch_idx + 1) / total_train_batches * 100\n",
    "            print(f\"  [{progress:5.1f}%] Batch {batch_idx+1:4d}/{total_train_batches} | \"\n",
    "                  f\"Loss: {loss.item():.4f} | Acc: {current_acc:.4f}\")\n",
    "\n",
    "    train_loss = running_loss / running_total\n",
    "    train_acc = running_correct / running_total\n",
    "    \n",
    "    # ============================================\n",
    "    # VALIDATION PHASE\n",
    "    # ============================================\n",
    "    print(f\"\\n[VALIDATION] Processing {len(val_loader)} batches...\")\n",
    "    \n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_correct = 0\n",
    "    running_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (batch_x, batch_y) in enumerate(val_loader):\n",
    "            batch_x = batch_x.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "\n",
    "            logits = model(batch_x)\n",
    "            loss = criterion(logits, batch_y)\n",
    "\n",
    "            running_loss += loss.item() * batch_x.size(0)\n",
    "\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            running_correct += (preds == batch_y).sum().item()\n",
    "            running_total += batch_x.size(0)\n",
    "    \n",
    "    val_loss = running_loss / running_total\n",
    "    val_acc = running_correct / running_total\n",
    "    \n",
    "    # ============================================\n",
    "    # SAVE METRICS & PRINT SUMMARY\n",
    "    # ============================================\n",
    "    history[\"train_loss\"].append(train_loss)\n",
    "    history[\"train_acc\"].append(train_acc)\n",
    "    history[\"val_loss\"].append(val_loss)\n",
    "    history[\"val_acc\"].append(val_acc)\n",
    "    \n",
    "    print(f\"\\n{'-'*70}\")\n",
    "    print(f\"[Epoch {epoch+1}/{EPOCHS} Summary]\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} ({train_acc*100:.2f}%)\")\n",
    "    print(f\"  Val Loss:   {val_loss:.4f}   | Val Acc:   {val_acc:.4f} ({val_acc*100:.2f}%)\")\n",
    "    \n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_state_dict = {k: v.cpu() for k, v in model.state_dict().items()}\n",
    "        print(f\"  New BEST model! (val_acc={best_val_acc:.4f})\")\n",
    "    else:\n",
    "        print(f\"  (Best val_acc so far: {best_val_acc:.4f})\")\n",
    "    \n",
    "    print(f\"{'-'*70}\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"=== Training Complete ===\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Best Validation Accuracy: {best_val_acc:.4f} ({best_val_acc*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c50d9f5-c6da-4ecf-8c1d-8605ee79253b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved best model -> best_custom_cnn_text_1.pth\n",
      "Saved vocab_text_1.json\n",
      "\n",
      "Training curves:\n",
      "Epoch 01: train_acc=0.9939, val_acc=0.9804, train_loss=0.0203, val_loss=0.1220\n",
      "Epoch 02: train_acc=0.9968, val_acc=0.9889, train_loss=0.0114, val_loss=0.0511\n",
      "Epoch 03: train_acc=0.9978, val_acc=0.9887, train_loss=0.0078, val_loss=0.0752\n",
      "Epoch 04: train_acc=0.9983, val_acc=0.9881, train_loss=0.0079, val_loss=0.0653\n",
      "Epoch 05: train_acc=0.9988, val_acc=0.9869, train_loss=0.0042, val_loss=0.1015\n",
      "Epoch 06: train_acc=0.9990, val_acc=0.9823, train_loss=0.0047, val_loss=0.1731\n",
      "Epoch 07: train_acc=0.9986, val_acc=0.9892, train_loss=0.0064, val_loss=0.0706\n",
      "Epoch 08: train_acc=0.9992, val_acc=0.9874, train_loss=0.0032, val_loss=0.1194\n",
      "Epoch 09: train_acc=0.9993, val_acc=0.9892, train_loss=0.0029, val_loss=0.0881\n",
      "Epoch 10: train_acc=0.9993, val_acc=0.9896, train_loss=0.0035, val_loss=0.1059\n"
     ]
    }
   ],
   "source": [
    "if best_state_dict is not None:\n",
    "    torch.save(best_state_dict, \"best_custom_cnn_text_1.pth\")\n",
    "    print(\"Saved best model -> best_custom_cnn_text_1.pth\")\n",
    "\n",
    "with open(\"vocab_text_1.json\", \"w\") as f:\n",
    "    json.dump({\"itos\": itos}, f, indent=2)\n",
    "print(\"Saved vocab_text_1.json\")\n",
    "\n",
    "print(\"\\nTraining curves:\")\n",
    "for i in range(EPOCHS):\n",
    "    print(\n",
    "        f\"Epoch {i+1:02d}: \"\n",
    "        f\"train_acc={history['train_acc'][i]:.4f}, \"\n",
    "        f\"val_acc={history['val_acc'][i]:.4f}, \"\n",
    "        f\"train_loss={history['train_loss'][i]:.4f}, \"\n",
    "        f\"val_loss={history['val_loss'][i]:.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8942bc-d55d-41ba-9ea0-62f9f4ba08db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (GPU)",
   "language": "python",
   "name": "my_gpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
